import { AxiosRequestConfig } from 'axios';
import { BaseLanguageModelCallOptions } from 'langchain/base_language';
import { CallbackManagerForLLMRun } from 'langchain/callbacks';
import { BaseChatModelParams } from 'langchain/chat_models/base';
import { ChatOpenAI } from 'langchain/chat_models/openai';
import { BaseMessage, ChatResult } from 'langchain/schema';
import { StructuredTool } from 'langchain/tools';
import { B as BTPBaseLLMParameters } from '../../base-cac7307b.js';
import { BTPLLMProxyChatCompletionParameters, BTPLLMChatModel, BTPLLMProxyFunction, BTPLLMProxyMessage, BTPLLMProxyChatCompletionResult, BTPLLMProxyFunctionCall } from '../../client/btp.js';
import '../../config-822cfdeb.js';
import '@sap-cloud-sdk/util';
import 'zod-to-json-schema/src/parseDef.js';
import '../../client/ai21.js';
import '../../client/alephalpha.js';
import '../../client/amazon.js';
import '../../client/anthropic.js';
import '../../client/cohere.js';
import '../../client/google.js';
import '../../client/huggingface.js';
import '../../client/openai.js';

/**
 * Input for Text generation for OpenAI
 */
interface BTPLLMProxyChatInput extends Omit<BTPLLMProxyChatCompletionParameters, "messages" | "functions">, BTPBaseLLMParameters<BTPLLMChatModel>, BaseChatModelParams {
}
/**
 * Chat Call options
 */
interface BTPLLMProxyChatCallOptions extends BaseLanguageModelCallOptions {
    functions?: BTPLLMProxyFunction[];
    function_call?: BTPLLMProxyFunctionCall;
    tools?: StructuredTool[];
    promptIndex?: number;
    options?: AxiosRequestConfig;
}
/**
 * BTP LLM Proxy Language Model Wrapper to generate texts
 *
 * **NOTE**: This is based on OpenAI's GPT langchain binding, since langchain has special handling for
 * OpenAI functions. Since BTP LLM Proxy also has a similar API interface, it's based on OpenAI's binding.
 * But still you can use it for any LLMs supported by BTP LLM Proxy.
 *
 * **NOTE**: You can even use text-only models here
 */
declare class BTPLLMProxyChat extends ChatOpenAI implements BTPLLMProxyChatInput {
    CallOptions: BTPLLMProxyChatCallOptions;
    private btpLLMProxyClient;
    deployment_id: BTPLLMChatModel;
    maxOutputTokens?: number;
    temperature: number;
    topP: number;
    topK?: number;
    n: number;
    stop?: string[];
    constructor(fields?: Partial<BTPLLMProxyChatInput>);
    get callKeys(): (keyof BTPLLMProxyChatCallOptions)[];
    get lc_secrets(): {
        [key: string]: string;
    } | undefined;
    get lc_aliases(): Record<string, string>;
    _llmType(): string;
    _generate(messages: BaseMessage[], options: this["CallOptions"], runManager?: CallbackManagerForLLMRun): Promise<ChatResult>;
    /**
     * Maps a LangChain {@link StructuredTool} to {@link BTPLLMProxyFunction}
     */
    protected mapToolToBTPLLMProxyFunction(tool: StructuredTool): BTPLLMProxyFunction;
    /**
     * Maps a {@link BaseMessage} to BTP LLM Proxy's Message Role
     */
    protected mapBaseMessageToRole(message: BaseMessage): BTPLLMProxyMessage["role"];
    /**
     * Maps {@link BaseMessage} to BTP LLM Proxy Messages
     */
    protected mapBaseMessagesToBTPLLMProxyMessages(messages: BaseMessage[], options: BTPLLMProxyChatCallOptions): BTPLLMProxyMessage[];
    /**
     * Maps BTP LLM Proxy messages to LangChain's {@link ChatResult}
     */
    protected mapBTPLLMProxyMessagesToChatResult(res: BTPLLMProxyChatCompletionResult): ChatResult;
}

export { BTPLLMProxyChat, type BTPLLMProxyChatInput };
