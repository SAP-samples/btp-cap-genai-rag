import { AxiosRequestConfig } from 'axios';
import { BaseLanguageModelCallOptions } from 'langchain/base_language';
import { CallbackManagerForLLMRun } from 'langchain/callbacks';
import { BaseLLMParams } from 'langchain/llms/base';
import { OpenAI } from 'langchain/llms/openai';
import { LLMResult } from 'langchain/schema';
import { B as BTPBaseLLMParameters } from '../../base-cac7307b.js';
import { BTPLLMProxyTextCompletionParameters, BTPLLMTextModel } from '../../client/btp.js';
import '../../config-822cfdeb.js';
import '@sap-cloud-sdk/util';
import 'zod-to-json-schema/src/parseDef.js';
import '../../client/ai21.js';
import '../../client/alephalpha.js';
import '../../client/amazon.js';
import '../../client/anthropic.js';
import '../../client/cohere.js';
import '../../client/google.js';
import '../../client/huggingface.js';
import '../../client/openai.js';

/**
 * Input for Text generation for BTP LLM Proxy
 */
interface BTPLLMProxyInput extends Omit<BTPLLMProxyTextCompletionParameters, "prompt" | "functions">, BTPBaseLLMParameters<BTPLLMTextModel>, BaseLLMParams {
}
/**
 * Call options
 */
interface BTPLLMProxyCallOptions extends BaseLanguageModelCallOptions {
    options?: AxiosRequestConfig;
}
/**
 * BTP LLM Proxy Language Model Wrapper to generate texts
 *
 * **NOTE**: This is based on OpenAI's GPT langchain binding, since BTP LLM Proxy has a similar API interface
 * that of OpenAI's binding. But still you can use it for any LLMs supported by BTP LLM Proxy.
 *
 * **NOTE**: You can even use chat-only models here
 */
declare class BTPLLMProxy extends OpenAI implements BTPLLMProxyInput {
    CallOptions: BTPLLMProxyCallOptions;
    private btpLLMProxyClient;
    deployment_id: BTPLLMTextModel;
    maxOutputTokens?: number;
    temperature: number;
    topP: number;
    topK?: number;
    n: number;
    stop?: string[];
    constructor(fields?: Partial<BTPLLMProxyInput>);
    get callKeys(): (keyof BTPLLMProxyCallOptions)[];
    get lc_secrets(): {
        [key: string]: string;
    } | undefined;
    get lc_aliases(): Record<string, string>;
    _llmType(): string;
    _generate(prompts: string[], options: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): Promise<LLMResult>;
}

export { BTPLLMProxy, type BTPLLMProxyInput };
