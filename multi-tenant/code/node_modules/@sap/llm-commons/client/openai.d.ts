import { AxiosRequestConfig } from 'axios';
import { B as BTPBaseLLMParameters, a as BTPLLMResult, b as BaseLLMClient, c as BTPLLMHttpClient } from '../base-cac7307b.js';
import { JsonSchema7Type } from 'zod-to-json-schema/src/parseDef.js';
import '../config-822cfdeb.js';
import '@sap-cloud-sdk/util';

/**
 * BTP OpenAI GPT Text Models
 */
type BTPOpenAIGPTTextModel = "text-davinci-003" | "code-davinci-002";
/**
 * BTP OpenAI GPT Chat Models
 */
type BTPOpenAIGPTChatModel = "gpt-35-turbo" | "gpt-35-turbo-16k" | "gpt-4" | "gpt-4-32k";
/**
 * BTP OpenAI GPT Embedding Models
 */
type BTPOpenAIGPTEmbeddingModel = "text-embedding-ada-002-v2";
/**
 * OpenAI GPT Chat Message
 */
type BTPOpenAIGPTMessage = {
    /**
     * Role of persona in this message
     */
    role: "system" | "user" | "assistant" | "function";
    /**
     * Name of the persona
     */
    name?: string;
    /**
     * Message content
     */
    content: string | null;
    /**
     * Function which is called
     */
    function_call?: BTPOpenAIGPTFunctionCall;
};
/**
 * BTP OpenAI GPT Function Signature
 */
type BTPOpenAIGPTFunction = {
    /**
     * Name of the function to be called
     */
    name: string;
    /**
     * Description of the function
     */
    description?: string;
    /**
     * JSON Schema for the function input parameters
     *
     * **Note**: As mentioned in {@link https://community.openai.com/t/whitch-json-schema-version-should-function-calling-use/283535/4}, it follows JSON Schema 7 (2020-12).
     *
     * @remarks Not all JSON Schema parameters in the specification are supported by OpenAI
     */
    parameters: JsonSchema7Type;
};
/**
 * BTP OpenAI GPT Function Call by AI
 */
type BTPOpenAIGPTFunctionCall = {
    /**
     * Name of the function call
     */
    name: string;
    /**
     * Input parameters for the function as serialized JSON
     */
    arguments: string;
};
/**
 * BTP LLM Base OpenAI GPT Completion Input Parameters
 */
interface BTPOpenAIGPTCompletionParameters {
    /**
     * The maximum number of tokens to generate in the completion. The token count of your prompt plus max_tokens can't exceed the model's context length. Most models have a context length of 2048 tokens (except for the newest models, which support 4096).
     */
    max_tokens?: number;
    /**
     * What sampling temperature to use, between 0 and 2. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (`argmax sampling`) for ones with a well-defined answer. We generally recommend altering this or top_p but not both.
     */
    temperature?: number;
    /**
     * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both.
     */
    top_p?: number;
    /**
     * Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. As an example, you can pass {"50256": -100} to prevent the <|endoftext|> token from being generated.
     */
    logit_bias?: Record<string, unknown>;
    /**
     * A unique identifier representing your end-user, which can help monitoring and detecting abuse
     */
    user?: string;
    /**
     * How many completions to generate for each prompt. Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop.
     */
    n?: number;
    /**
     * Up to four sequences where the API will stop generating further tokens. The returned text won't contain the stop sequence.
     */
    stop?: string | string[];
    /**
     * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
     */
    presence_penalty?: number;
    /**
     * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
     */
    frequency_penalty?: number;
}
/**
 * BTP LLM OpenAI GPT Text Completion Input Parameters
 */
interface BTPOpenAIGPTTextCompletionParameters extends BTPBaseLLMParameters<BTPOpenAIGPTTextModel>, BTPOpenAIGPTCompletionParameters {
    /**
     * The prompt(s) to generate completions for, encoded as a string, or array of strings. Note that `<\|endoftext\|>` is the document separator that the model sees during training, so if a prompt isn't specified the model will generate as if from the beginning of a new document.
     */
    prompt: string | string[];
    /**
     * Include the log probabilities on the logprobs most likely tokens, as well the chosen tokens. For example, if logprobs is 10, the API will return a list of the 10 most likely tokens. the API will always return the logprob of the sampled token, so there may be up to logprobs+1 elements in the response. This parameter cannot be used with `gpt-35-turbo`.
     */
    logprobs?: number;
    /**
     * The suffix that comes after a completion of inserted text.
     */
    suffix?: string;
    /**
     * 	Echo back the prompt in addition to the completion. This parameter cannot be used with `gpt-35-turbo`.
     */
    echo?: boolean;
    /**
     * Generates best_of completions server-side and returns the "best" (the one with the lowest log probability per token). Results can't be streamed. When used with n, best_of controls the number of candidate completions and n specifies how many to return â€“ best_of must be greater than n. Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop. This parameter cannot be used with `gpt-35-turbo`.
     */
    best_of?: number;
}
/**
 * BTP LLM OpenAI GPT Chat Completion Input Parameters
 */
interface BTPOpenAIGPTChatCompletionParameters extends BTPBaseLLMParameters<BTPOpenAIGPTChatModel>, BTPOpenAIGPTCompletionParameters {
    /**
     * An array of system, user & assistant messages for chat completion
     */
    messages: BTPOpenAIGPTMessage[];
    /**
     * Array of function signatures to be called
     */
    functions?: BTPOpenAIGPTFunction[];
}
/**
 * BTP LLM OpenAI GPT Embedding Input Parameters
 */
interface BTPOpenAIGPTEmbeddingParameters extends BTPBaseLLMParameters<BTPOpenAIGPTEmbeddingModel> {
    /**
     * Input text to get embeddings for, encoded as a string. The number of input tokens varies depending on what model you are using. Unless you're embedding code, we suggest replacing newlines (\n) in your input with a single space, as we have observed inferior results when newlines are present.
     */
    input: Array<number> | Array<string> | string;
    /**
     * A unique identifier representing for your end-user. This will help Azure OpenAI monitor and detect abuse. Do not pass PII identifiers instead use pseudoanonymized values such as GUIDs
     */
    user?: string;
}
/**
 * BTP LLM OpenAI GPT Completion Output Result
 */
interface BTPOpenAIGPTLLMOutput extends BTPLLMResult {
    /**
     *
     */
    created: number;
    /**
     * Unique ID for completion
     */
    id: string;
    /**
     * Name of the model used for completion
     */
    model: string;
    /**
     * Completion object
     */
    object: "chat.completion" | "text_completion";
    /**
     * Token Usage
     */
    usage: {
        /**
         * Tokens consumed for output text completion
         */
        completion_tokens: number;
        /**
         * Tokens consumed for input prompt tokens
         */
        prompt_tokens: number;
        /**
         * Total tokens consumed for input + output
         */
        total_tokens: number;
    };
}
/**
 * BTP LLM OpenAI GPT Text Completion Output Result
 */
interface BTPOpenAIGPTCompletionResult extends BTPOpenAIGPTLLMOutput {
    /**
     * Array of result candidates
     */
    choices: {
        /**
         * Reason for finish
         */
        finish_reason: string | null;
        /**
         * Index of choice
         */
        index: number;
        /**
         * Log probabilities
         */
        logprobs?: object;
        /**
         * Completion text
         */
        text: string;
    }[];
}
/**
 * BTP LLM OpenAI GPT Chat Completion Output Result
 */
interface BTPOpenAIGPTChatCompletionResult extends BTPOpenAIGPTLLMOutput {
    /**
     * Array of result candidates
     */
    choices: {
        /**
         * Reason for finish
         */
        finish_reason: string | null;
        /**
         * Index of choice
         */
        index: number;
        /**
         * Log probabilities
         */
        logprobs?: object;
        /**
         * Completion chat message
         */
        message: BTPOpenAIGPTMessage;
    }[];
}
/**
 * BTP LLM OpenAI GPT Embedding Output Result
 */
interface BTPOpenAIGPTEmbeddingResult extends BTPLLMResult {
    /**
     * List object
     */
    object: "list";
    /**
     * Model used for embedding
     */
    model: string;
    /**
     * Array of result candidates
     */
    data: [
        {
            /**
             * Embedding object
             */
            object: "embedding";
            /**
             * Array of size `1536` (OpenAI's embedding size) containing embedding vector
             */
            embedding: number[];
            /**
             * Index of choice
             */
            index: number;
        }
    ];
    /**
     * Token Usage
     */
    usage: {
        /**
         * Tokens consumed for input prompt tokens
         */
        prompt_tokens: number;
        /**
         * Total tokens consumed
         */
        total_tokens: number;
    };
}
/**
 * BTP LLM OpenAI GPT Client
 */
declare class BTPOpenAIGPTClient extends BaseLLMClient {
    /**
     * @param httpClient HTTP Client
     */
    constructor(httpClient?: BTPLLMHttpClient);
    /**
     * Creates a Text Completion
     *
     * @param params the payload to send to the API
     * @param requestConfig Axios Request Configuration
     *
     * @see https://axios-http.com/docs/req_config
     * @see https://axios-http.com/docs/cancellation
     *
     * @returns HTTP Response data as JSON
     * @throws {@link BTPLLMError} in case of HTTP errors
     */
    createCompletion(params: BTPOpenAIGPTTextCompletionParameters, requestConfig?: AxiosRequestConfig): Promise<BTPOpenAIGPTCompletionResult>;
    /**
     * Creates a Chat Completion
     *
     * @param params the payload to send to the API
     * @param requestConfig Axios Request Configuration
     *
     * @see https://axios-http.com/docs/req_config
     * @see https://axios-http.com/docs/cancellation
     *
     * @returns HTTP Response data as JSON
     * @throws {@link BTPLLMError} in case of HTTP errors
     */
    createChatCompletion(params: BTPOpenAIGPTChatCompletionParameters, requestConfig?: AxiosRequestConfig): Promise<BTPOpenAIGPTChatCompletionResult>;
    /**
     * Creates an Embedding
     *
     * @param params the payload to send to the API
     * @param requestConfig Axios Request Configuration
     *
     * @see https://axios-http.com/docs/req_config
     * @see https://axios-http.com/docs/cancellation
     *
     * @returns HTTP Response data as JSON
     * @throws {@link BTPLLMError} in case of HTTP errors
     */
    createEmbedding(params: BTPOpenAIGPTEmbeddingParameters, requestConfig?: AxiosRequestConfig): Promise<BTPOpenAIGPTEmbeddingResult>;
}
/**
 * @deprecated Use {@link BTPOpenAIGPTClient} instead
 */
declare const BTPOpenAIClient: typeof BTPOpenAIGPTClient;

export { BTPOpenAIClient, type BTPOpenAIGPTChatCompletionParameters, type BTPOpenAIGPTChatCompletionResult, type BTPOpenAIGPTChatModel, BTPOpenAIGPTClient, type BTPOpenAIGPTCompletionResult, type BTPOpenAIGPTEmbeddingModel, type BTPOpenAIGPTEmbeddingParameters, type BTPOpenAIGPTEmbeddingResult, type BTPOpenAIGPTFunction, type BTPOpenAIGPTFunctionCall, type BTPOpenAIGPTMessage, type BTPOpenAIGPTTextCompletionParameters, type BTPOpenAIGPTTextModel };
