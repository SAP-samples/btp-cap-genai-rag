import { AxiosRequestConfig } from 'axios';
import { JsonSchema7Type } from 'zod-to-json-schema/src/parseDef.js';
import { B as BTPBaseLLMParameters, a as BTPLLMResult, b as BaseLLMClient, c as BTPLLMHttpClient } from '../base-cac7307b.js';
import { BTPAI21JurassicTextModel } from './ai21.js';
import { BTPAlephAlphaLuminousTextModel } from './alephalpha.js';
import { BTPAmazonTitanTextModel, BTPAmazonTitanEmbeddingModel } from './amazon.js';
import { BTPAnthropicClaudeChatModel } from './anthropic.js';
import { BTPCohereCommandTextModel } from './cohere.js';
import { BTPGooglePaLMTextModel, BTPGooglePaLMChatModel, BTPGooglePaLMEmbeddingModel } from './google.js';
import { BTPHuggingFaceTextModel } from './huggingface.js';
import { BTPOpenAIGPTTextModel, BTPOpenAIGPTChatModel, BTPOpenAIGPTEmbeddingModel } from './openai.js';
import '../config-822cfdeb.js';
import '@sap-cloud-sdk/util';

/**
 * BTP LLM Supported Text Models
 */
type BTPLLMTextModel = BTPOpenAIGPTTextModel | BTPOpenAIGPTChatModel | BTPGooglePaLMTextModel | BTPGooglePaLMChatModel | BTPAnthropicClaudeChatModel | BTPCohereCommandTextModel | BTPAmazonTitanTextModel | BTPAlephAlphaLuminousTextModel | BTPAI21JurassicTextModel | BTPHuggingFaceTextModel;
/**
 * BTP LLM Supported Chat Models
 */
type BTPLLMChatModel = BTPOpenAIGPTTextModel | BTPOpenAIGPTChatModel | BTPGooglePaLMTextModel | BTPGooglePaLMChatModel | BTPAnthropicClaudeChatModel | BTPCohereCommandTextModel | BTPAmazonTitanTextModel | BTPAlephAlphaLuminousTextModel | BTPAI21JurassicTextModel | BTPHuggingFaceTextModel;
/**
 * BTP LLM Supported Embedding Models
 */
type BTPLLMEmbeddingModel = BTPOpenAIGPTEmbeddingModel | BTPGooglePaLMEmbeddingModel | BTPAmazonTitanEmbeddingModel;
interface BTPLLMProxyFunction {
    /**
     * The name of the function to be called. It should contain a-z, A-Z, 0-9, underscores and dashes, with a maximum
     * length of 64 characters.
     */
    name: string;
    /**
     * A description explaining what the function does. It helps the model to decide when and how to call the function.
     */
    description?: string;
    /**
     * The parameters that the function accepts, described as a JSON Schema object.
     */
    parameters: JsonSchema7Type;
}
/**
 * BTP LLM Proxy Function Call by AI
 */
type BTPLLMProxyFunctionCall = {
    /**
     * Name of the function call
     */
    name: string;
    /**
     * Input parameters for the function as serialized JSON
     */
    arguments: string;
};
/**
 * BTP LLM Proxy Chat Message
 */
interface BTPLLMProxyMessage {
    /**
     * The role of the message's author
     */
    role: "system" | "user" | "assistant" | "function";
    /**
     * The name of the author of the message. It is required if the role is "function". The name should match the name of the function represented in the content. It can contain characters (a-z, A-Z, 0-9), and underscores, with a maximum length of 64 characters.
     */
    name?: string;
    /**
     * The contents of the message. It is required for all messages, but may be null for assistant messages with function calls.
     */
    content: string | null;
    /**
     * The name and arguments of a function that should be called, as generated by the model.
     */
    functionCall?: BTPLLMProxyFunctionCall;
}
/**
 * BTP LLM Proxy Base Input Parameters
 */
interface BTPLLMProxyParameters {
    /**
     * The temperature is used for sampling during response generation. Temperature controls the degree of randomness in
     * token selection. Lower temperatures are good for prompts that require a more deterministic and less open-ended or
     * creative response, while higher temperatures can lead to more diverse or creative results.
     *
     * A temperature of 0 is deterministic, meaning that the highest probability response is always selected.
     *
     * For most use cases, try starting with a temperature of 0.2. If the model returns a response that's too generic,
     * too short, or the model gives a fallback response, try increasing the temperature.
     */
    temperature?: number;
    /**
     * Maximum number of tokens that can be generated in the response.
     *
     * Specify a lower value for shorter responses and a higher value for longer responses.
     */
    maxOutputTokens?: number;
    /**
     * Top-P changes how the model selects tokens for output.
     *
     * For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the top-P value is 0.5, then the model
     * will select either A or B as the next token by using temperature and excludes C as a candidate.
     *
     * Specify a lower value for less random responses and a higher value for more random responses.
     */
    topP?: number;
    /**
     * Top-K changes how the model selects tokens for output.
     *
     * For each token selection step, the top-K tokens with the highest probabilities are sampled. Then tokens are further
     * filtered based on top-P with the final token selected using temperature sampling.
     *
     * Specify a lower value for less random responses and a higher value for more random responses.
     */
    topK?: number;
    /**
     * How many completions to generate for each prompt.
     *
     * **Note**: Because this parameter generates many completions, it can quickly consume your token quota.
     * Use carefully and ensure that you have reasonable settings for `maxOutputTokens` and `stop`.
     */
    n?: number;
    /**
     * Character sequences to stop generating further tokens.
     */
    stop?: string[];
}
/**
 * BTP LLM Proxy Text Completion Input Parameters
 */
interface BTPLLMProxyTextCompletionParameters extends BTPBaseLLMParameters<BTPLLMTextModel>, BTPLLMProxyParameters {
    /**
     * The prompt to generate completions for, encoded as a string
     */
    prompt: string;
    /**
     * A list of functions that the model may use to generate JSON inputs.
     */
    functions?: BTPLLMProxyFunction[];
}
/**
 * BTP LLM Proxy Chat Completion Input Parameters
 */
interface BTPLLMProxyChatCompletionParameters extends BTPBaseLLMParameters<BTPLLMChatModel>, BTPLLMProxyParameters {
    /**
     * An array of system, user & assistant messages for chat completion
     */
    messages: BTPLLMProxyMessage[];
    /**
     * A list of functions that the model may use to generate JSON inputs.
     */
    functions?: BTPLLMProxyFunction[];
}
/**
 * BTP LLM Proxy Embedding Input Parameters
 */
interface BTPLLMProxyEmbeddingParameters extends BTPBaseLLMParameters<BTPLLMEmbeddingModel> {
    /**
     * Input text to get embeddings for, encoded as a string. The number of input tokens varies depending on what model you
     * are using.
     *
     * **Note**: Unless you're embedding code, we suggest replacing newlines (\n) in your input with a single space, as we
     * have observed inferior results when newlines are present.
     */
    input: string;
}
interface BTPLLMProxyCompletionResult extends BTPLLMResult {
    /**
     * Created time
     */
    created: number;
    /**
     * Name of the model used for completion
     */
    model: string;
    /**
     * Token Usage
     */
    usage?: {
        /**
         * Tokens consumed for output text completion
         */
        completionTokens?: number;
        /**
         * Tokens consumed for input prompt tokens
         */
        promptTokens?: number;
        /**
         * Total tokens consumed for input + output
         */
        totalTokens?: number;
    };
}
interface BTPLLMProxyTextCompletionResult extends BTPLLMProxyCompletionResult {
    /**
     * Completed choices
     */
    choices: {
        /**
         * Index of choice
         */
        index: number;
        /**
         * Reason for finish
         */
        finishReason?: string;
        /**
         * Completed chat message
         */
        text: string;
    }[];
}
interface BTPLLMProxyChatCompletionResult extends BTPLLMProxyCompletionResult {
    /**
     * Completed choices
     */
    choices: {
        /**
         * Index of choice
         */
        index: number;
        /**
         * Reason for finish
         */
        finishReason?: string;
        /**
         * Completed chat message
         */
        message: BTPLLMProxyMessage;
    }[];
}
/**
 * BTP LLM Proxy Embedding Output Result
 */
interface BTPLLMProxyEmbeddingResult extends BTPLLMResult {
    /**
     * Model used for embedding
     */
    model: string;
    /**
     * Array of result candidates
     */
    data: {
        /**
         * Index of choice
         */
        index: number;
        /**
         * Embedding vector
         */
        embedding: number[];
    }[];
    /**
     * Token Usage
     */
    usage?: {
        /**
         * Tokens consumed for input prompt
         */
        promptTokens: number;
    };
}
/**
 * BTP LLM Proxy Harmonized Client
 */
declare class BTPLLMProxyClient extends BaseLLMClient {
    /**
     * @param httpClient HTTP Client
     */
    constructor(httpClient?: BTPLLMHttpClient);
    /**
     * Creates a Text Completion
     *
     * **Note**: This is just a wrapper around text/chat completion with underlying LLM client.
     *
     * @param params the payload to send to the API
     * @param requestConfig Axios Request Configuration
     *
     * @see https://axios-http.com/docs/req_config
     * @see https://axios-http.com/docs/cancellation
     *
     * @returns HTTP Response data as JSON
     * @throws {@link BTPLLMError} in case of HTTP errors or unknown `deployment_id`
     */
    createTextCompletion(params: BTPLLMProxyTextCompletionParameters, requestConfig?: AxiosRequestConfig<BTPLLMProxyTextCompletionParameters>): Promise<BTPLLMProxyTextCompletionResult>;
    /**
     * Creates a Chat Completion
     *
     * **Note**: This is just a wrapper around text/chat completion with underlying LLM client.
     *
     * @param params the payload to send to the API
     * @param requestConfig Axios Request Configuration
     *
     * @see https://axios-http.com/docs/req_config
     * @see https://axios-http.com/docs/cancellation
     *
     * @returns HTTP Response data as JSON
     * @throws {@link BTPLLMError} in case of HTTP errors or unknown `deployment_id`
     */
    createChatCompletion(params: BTPLLMProxyChatCompletionParameters, requestConfig?: AxiosRequestConfig<BTPLLMProxyChatCompletionParameters>): Promise<BTPLLMProxyChatCompletionResult>;
    /**
     * Creates an Embedding
     *
     * **Note**: This is just a wrapper around embedding with underlying LLM client.
     *
     * @param params the payload to send to the API
     * @param requestConfig Axios Request Configuration
     *
     * @see https://axios-http.com/docs/req_config
     * @see https://axios-http.com/docs/cancellation
     *
     * @returns HTTP Response data as JSON
     * @throws {@link BTPLLMError} in case of HTTP errors
     */
    createEmbedding(params: BTPLLMProxyEmbeddingParameters, requestConfig?: AxiosRequestConfig): Promise<BTPLLMProxyEmbeddingResult>;
    /**
     * Creates a Text completion via OpenAI GPT client
     */
    private createTextCompletionViaOpenAIGPTClient;
    /**
     * Creates a Text completion via Google PaLM client
     */
    private createTextCompletionViaGooglePaLMClient;
    /**
     * Creates a Text completion via Anthropic Claude client
     */
    private createTextCompletionViaAnthropicClaudeClient;
    /**
     * Creates a Text completion via Cohere Command client
     */
    private createTextCompletionViaCohereCommandClient;
    /**
     * Creates a Text completion via Amazon Titan client
     */
    private createTextCompletionViaAmazonTitanClient;
    /**
     * Creates a Text completion via AlephAlpha Luminous client
     */
    private createTextCompletionViaAlephAlphaLuminousClient;
    /**
     * Creates a Text completion via AI21 Jurassic client
     */
    private createTextCompletionViaAI21JurassicClient;
    /**
     * Creates a Text completion via HuggingFace client
     */
    private createTextCompletionViaHuggingFaceClient;
    /**
     * Creates a Chat completion via OpenAI GPT client
     */
    private createChatCompletionViaOpenAIGPTClient;
    /**
     * Creates a Chat completion via Google PaLM client
     */
    private createChatCompletionViaGooglePaLMClient;
    /**
     * Creates a Chat completion via Anthropic Claude client
     */
    private createChatCompletionViaAnthropicClaudeClient;
    /**
     * Creates a Chat completion via Cohere Command client
     */
    private createChatCompletionViaCohereCommandClient;
    /**
     * Creates a Chat completion via Amazon Titan client
     */
    private createChatCompletionViaAmazonTitanClient;
    /**
     * Creates a Chat completion via AlephAlpha Luminous client
     */
    private createChatCompletionViaAlephAlphaLuminousClient;
    /**
     * Creates a Chat completion via AI21 Jurassic client
     */
    private createChatCompletionViaAI21JurassicClient;
    /**
     * Creates a Chat completion via HuggingFace client
     */
    private createChatCompletionViaHuggingFaceClient;
    /**
     * Creates embedding via OpenAI GPT client
     */
    private createEmbeddingViaOpenAIGPTClient;
    /**
     * Creates embedding via Google PaLM client
     */
    private createEmbeddingViaGooglePaLMClient;
    /**
     * Creates embedding via Amazon Titan client
     */
    private createEmbeddingViaAmazonTitanClient;
}

export { type BTPLLMChatModel, type BTPLLMEmbeddingModel, type BTPLLMProxyChatCompletionParameters, type BTPLLMProxyChatCompletionResult, BTPLLMProxyClient, type BTPLLMProxyEmbeddingParameters, type BTPLLMProxyEmbeddingResult, type BTPLLMProxyFunction, type BTPLLMProxyFunctionCall, type BTPLLMProxyMessage, type BTPLLMProxyTextCompletionParameters, type BTPLLMProxyTextCompletionResult, type BTPLLMTextModel };
