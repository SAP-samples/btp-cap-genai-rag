import { AxiosRequestConfig } from 'axios';
import { B as BTPBaseLLMParameters, a as BTPLLMResult, b as BaseLLMClient, c as BTPLLMHttpClient } from '../base-cac7307b.js';
import '../config-822cfdeb.js';
import '@sap-cloud-sdk/util';

/**
 * BTP Cohere Command Text Models
 */
type BTPCohereCommandTextModel = "cohere-command";
/**
 * BTP LLM Cohere Command Text Completion Input Parameters
 */
interface BTPCohereCommandTextCompletionParameters extends BTPBaseLLMParameters<BTPCohereCommandTextModel> {
    /**
     * The input text that serves as the starting point for generating the response.
     *
     * **Note**: The prompt will be pre-processed and modified before reaching the model.
     */
    prompt: string;
    /**
     * The maximum number of generations that will be returned. Defaults to `1`, min value of `1`, max value of `5`.
     */
    num_generations?: number;
    /**
     * The maximum number of tokens the model will generate as part of the response. Note: Setting a low value may result
     * in incomplete generations.
     *
     * Can only be set to `0` if `return_likelihoods` is set to `ALL` to get the likelihood of the prompt.
     *
     * @default 20
     */
    max_tokens?: number;
    /**
     * One of `NONE|START|END` to specify how the API will handle inputs longer than the maximum token length.
     *
     * Passing `START` will discard the start of the input. `END` will discard the end of the input. In both cases, input is
     * discarded until the remaining input is exactly the maximum input token length for the model.
     *
     * If `NONE` is selected, when the input exceeds the maximum input token length an error will be returned.
     *
     * @default END
     */
    truncate?: "NONE" | "START" | "END";
    /**
     * A non-negative float that tunes the degree of randomness in generation. Lower temperatures mean less random generations.
     *
     * Allowed values: min value of 0.0, max value of 5.0.
     *
     * @default 0.75
     */
    temperature?: number;
    /**
     * The generated text will be cut at the beginning of the earliest occurence of an end sequence. The sequence will be excluded
     * from the text.
     */
    end_sequences?: string[];
    /**
     * The generated text will be cut at the end of the earliest occurence of a stop sequence. The sequence will be included the text.
     */
    stop_sequences?: string[];
    /**
     * Ensures only the top `k` most likely tokens are considered for generation at each step.
     *
     * Allowed values: min value of 0, max value of 500.
     *
     * @default 0
     */
    k?: number;
    /**
     * Ensures that only the most likely tokens, with total probability mass of `p`, are considered for generation at each step.
     * If both `k` and `p` are enabled, `p` acts after `k`.
     *
     * Allowed values: min value of 0.01, max value of 0.99.
     *
     * @default 0
     */
    p?: number;
    /**
     * Used to reduce repetitiveness of generated tokens. The higher the value, the stronger a penalty is applied to previously
     * present tokens, proportional to how many times they have already appeared in the prompt or prior generation.
     *
     * Allowed values: min value of 0.0, max value of 1.0.
     */
    frequency_penalty?: number;
    /**
     * Can be used to reduce repetitiveness of generated tokens. Similar to `frequency_penalty`, except that this penalty is
     * applied equally to all tokens that have already appeared, regardless of their exact frequencies.
     *
     * Allowed values: min value of 0.0, max value of 1.0.
     *
     * @default 0.0
     */
    presence_penalty?: number;
    /**
     * One of `GENERATION|ALL|NONE` to specify how and if the token likelihoods are returned with the response. Defaults to `NONE`.
     *
     * If `GENERATION` is selected, the token likelihoods will only be provided for generated text.
     *
     * If `ALL` is selected, the token likelihoods will be provided both for the prompt and the generated text.
     *
     * @default NONE
     */
    return_likelihoods?: "GENERATION" | "ALL" | "NONE";
    /**
     * Used to prevent the model from generating unwanted tokens or to incentivize it to include desired tokens. The format is
     * `{token_id: bias}` where bias is a float between -10 and 10. Tokens can be obtained from text using Tokenize.
     *
     * For example, if the value `{'11': -10}` is provided, the model will be very unlikely to include the token 11 (`"\n"`,
     * the newline character) anywhere in the generated text. In contrast `{'11': 10}` will result in generations that nearly
     * only contain that token. Values between -10 and 10 will proportionally affect the likelihood of the token appearing in
     * the generated text.
     */
    logit_bias?: Record<string, string>;
}
/**
 * BTP LLM Cohere Command Completion Output Result
 */
interface BTPCohereCommandTextCompletionResult extends BTPLLMResult {
    /**
     * Text Generation ID
     */
    id: string;
    /**
     * Prompt used for generations.
     */
    prompt?: string;
    /**
     * List of generated results
     */
    generations: {
        /**
         * Text Generation ID
         */
        id: string;
        /**
         * Generated Text
         */
        text: string;
        /**
         * Refers to the nth generation. Only present when `num_generations` is greater than zero.
         */
        index?: number;
        /**
         * Likelihood
         */
        likelihood?: number;
        /**
         * Only returned if `return_likelihoods` is set to `GENERATION` or `ALL`. The likelihood refers to the average log-likelihood
         * of the entire specified string, which is useful for evaluating the performance of your model, especially if you've created
         * a custom model. Individual token likelihoods provide the log-likelihood of each token. The first token will not have a
         * likelihood.
         */
        token_likelihoods?: {
            /**
             * Token
             */
            token: string;
            /**
             * Likelihood
             */
            likelihood: number;
        }[];
    }[];
    /**
     * Generation metadata
     */
    meta?: {
        /**
         * API version
         */
        api_version?: {
            /**
             * API Version
             */
            version: string;
            /**
             * API deprecated
             */
            is_deprecated?: boolean;
            /**
             * API experimental
             */
            is_experimental?: boolean;
        };
        /**
         * Warnings
         */
        warnings: string[];
    };
}
/**
 * BTP LLM Cohere Command Client
 */
declare class BTPCohereCommandClient extends BaseLLMClient {
    /**
     * @param httpClient HTTP Client
     */
    constructor(httpClient?: BTPLLMHttpClient);
    /**
     * Creates a Text Completion
     *
     * @param params the payload to send to the API
     * @param requestConfig Axios Request Configuration
     *
     * @see https://axios-http.com/docs/req_config
     * @see https://axios-http.com/docs/cancellation
     *
     * @returns HTTP Response data as JSON
     * @throws {@link BTPLLMError} in case of HTTP errors
     */
    createTextCompletion(params: BTPCohereCommandTextCompletionParameters, requestConfig?: AxiosRequestConfig): Promise<BTPCohereCommandTextCompletionResult>;
}

export { BTPCohereCommandClient, type BTPCohereCommandTextCompletionParameters, type BTPCohereCommandTextCompletionResult, type BTPCohereCommandTextModel };
