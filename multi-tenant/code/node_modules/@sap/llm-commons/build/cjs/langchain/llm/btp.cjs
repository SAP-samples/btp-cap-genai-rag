'use strict';

var openai = require('langchain/llms/openai');
var client_btp = require('../../client/btp.cjs');
require('../../index-2134ea38.cjs');
require('@sap-cloud-sdk/util');
require('@sap/xsenv');
require('fs');
require('util');
require('../../utils-b80ecc7f.cjs');
require('../../client/ai21.cjs');
require('../../base-921f7eb6.cjs');
require('axios');
require('../../client/alephalpha.cjs');
require('../../client/amazon.cjs');
require('../../client/anthropic.cjs');
require('../../client/cohere.cjs');
require('../../client/google.cjs');
require('../../client/huggingface.cjs');
require('../../client/openai.cjs');

var __defProp = Object.defineProperty;
var __defProps = Object.defineProperties;
var __getOwnPropDescs = Object.getOwnPropertyDescriptors;
var __getOwnPropSymbols = Object.getOwnPropertySymbols;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __propIsEnum = Object.prototype.propertyIsEnumerable;
var __defNormalProp = (obj, key, value) => key in obj ? __defProp(obj, key, { enumerable: true, configurable: true, writable: true, value }) : obj[key] = value;
var __spreadValues = (a, b) => {
  for (var prop in b || (b = {}))
    if (__hasOwnProp.call(b, prop))
      __defNormalProp(a, prop, b[prop]);
  if (__getOwnPropSymbols)
    for (var prop of __getOwnPropSymbols(b)) {
      if (__propIsEnum.call(b, prop))
        __defNormalProp(a, prop, b[prop]);
    }
  return a;
};
var __spreadProps = (a, b) => __defProps(a, __getOwnPropDescs(b));
var __publicField = (obj, key, value) => {
  __defNormalProp(obj, typeof key !== "symbol" ? key + "" : key, value);
  return value;
};
class BTPLLMProxy extends openai.OpenAI {
  constructor(fields) {
    var _a, _b, _c, _d;
    super(__spreadProps(__spreadValues({}, fields), { openAIApiKey: "dummy" }));
    __publicField(this, "btpLLMProxyClient");
    __publicField(this, "deployment_id");
    __publicField(this, "maxOutputTokens");
    __publicField(this, "topK");
    this.deployment_id = (_a = fields == null ? void 0 : fields.deployment_id) != null ? _a : "gpt-35-turbo";
    this.temperature = (_b = fields == null ? void 0 : fields.temperature) != null ? _b : this.temperature;
    this.maxOutputTokens = fields == null ? void 0 : fields.maxOutputTokens;
    this.topP = (_c = fields == null ? void 0 : fields.topP) != null ? _c : this.topP;
    this.topK = fields == null ? void 0 : fields.topK;
    this.n = (_d = fields == null ? void 0 : fields.n) != null ? _d : 1;
    this.stop = fields == null ? void 0 : fields.stop;
    this.btpLLMProxyClient = new client_btp.BTPLLMProxyClient();
  }
  get callKeys() {
    return [...super.callKeys, "options"];
  }
  get lc_secrets() {
    return {};
  }
  get lc_aliases() {
    return {};
  }
  _llmType() {
    return "btp-llm-proxy";
  }
  async _generate(prompts, options, runManager) {
    var _a, _b, _c;
    const res = await this.caller.callWithOptions(
      {
        signal: options.signal
      },
      () => {
        var _a2, _b2;
        return this.btpLLMProxyClient.createTextCompletion(
          __spreadValues({
            prompt: prompts.length > 0 ? prompts[0] : "",
            // TODO: fix how to handle multiple prompts
            deployment_id: this.deployment_id,
            maxOutputTokens: this.maxTokens === -1 ? void 0 : this.maxTokens,
            temperature: this.temperature,
            topP: this.topP,
            topK: this.topK,
            n: this.n,
            stop: (_a2 = options == null ? void 0 : options.stop) != null ? _a2 : this.stop
          }, this.modelKwargs),
          {
            signal: options.signal,
            timeout: (_b2 = this.timeout) != null ? _b2 : options.timeout
          }
        );
      }
    );
    await (runManager == null ? void 0 : runManager.handleLLMNewToken(res.choices[0].text));
    return {
      generations: res.choices.map((c) => [
        {
          text: c.text,
          generationInfo: {
            finish_reason: c.finishReason,
            index: c.index
          }
        }
      ]),
      llmOutput: {
        created: res.created,
        model: res.model,
        tokenUsage: {
          completionTokens: (_a = res.usage) == null ? void 0 : _a.completionTokens,
          promptTokens: (_b = res.usage) == null ? void 0 : _b.promptTokens,
          totalTokens: (_c = res.usage) == null ? void 0 : _c.totalTokens
        }
      }
    };
  }
}

exports.BTPLLMProxy = BTPLLMProxy;
