import { AxiosRequestConfig } from 'axios';
import { B as BTPBaseLLMParameters, a as BTPLLMResult, b as BaseLLMClient, c as BTPLLMHttpClient } from '../base-cac7307b.js';
import '../config-822cfdeb.js';
import '@sap-cloud-sdk/util';

/**
 * BTP Anthropic Claude Text Models
 */
type BTPAnthropicClaudeChatModel = "anthropic-claude-v1" | "anthropic-claude-v1-100k" | "anthropic-claude-instant-v1" | "anthropic-claude-v2" | "anthropic-claude-v2-100k";
/**
 * Prefix for Anthropic Claude Human Prompt
 */
declare const ANTHROPIC_HUMANPROMPT = "\n\nHuman:";
/**
 * Prefix for Anthropic Claude AI Prompt
 */
declare const ANTHROPIC_AIPROMPT = "\n\nAssistant:";
/**
 * Anthropic Claude Chat Message
 */
type BTPAnthropicClaudeChatMessage = {
    /**
     * The role of the message creator
     */
    role: "Human" | "Assistant";
    /**
     * The content of the message
     */
    content: string;
};
/**
 * Base BTP LLM Anthropic Claude Completion Parameters
 */
interface BTPAnthropicClaudeParameters {
    /**
     * The maximum number of tokens to generate before stopping.
     *
     * Note that our models may stop before reaching this maximum. This parameter only specifies the absolute maximum number of tokens to generate.
     */
    max_tokens_to_sample: number;
    /**
     * Sequences that will cause the model to stop generating completion text.
     *
     * Our models stop on `"\n\nHuman:"`, and may include additional built-in stop sequences in the future. By providing the stop_sequences parameter,
     * you may include additional strings that will cause the model to stop generating.
     */
    stop_sequences?: string[];
    /**
     * Amount of randomness injected into the response.
     *
     * Ranges from 0 to 1. Use temp closer to 0 for analytical / multiple choice, and closer to 1 for creative and generative tasks.
     *
     * @default 1
     */
    temperature?: number;
    /**
     * Use nucleus sampling.
     *
     * In nucleus sampling, we compute the cumulative distribution over all the options for each subsequent token in decreasing probability order
     * and cut it off once it reaches a particular probability specified by `top_p`. You should either alter `temperature` or `top_p`, but not both.
     */
    top_p?: number;
    /**
     * Only sample from the top K options for each subsequent token.
     *
     * Used to remove "long tail" low probability responses.
     */
    top_k?: number;
}
/**
 * Anthropic Claude Text Completion Parameters
 */
interface BTPAnthropicClaudeTextParameters extends BTPBaseLLMParameters<BTPAnthropicClaudeChatModel>, BTPAnthropicClaudeParameters {
    /**
     * The prompt that you want Claude to complete.
     */
    prompt: string;
}
/**
 * Anthropic Claude Chat Completion Parameters
 */
interface BTPAnthropicClaudeChatParameters extends BTPBaseLLMParameters<BTPAnthropicClaudeChatModel>, BTPAnthropicClaudeParameters {
    /**
     * The prompt that you want Claude to complete.
     */
    messages: BTPAnthropicClaudeChatMessage[];
}
/**
 * Base BTP LLM Anthropic Claude Completion Output Result
 */
interface BTPAnthropicClaudeCompletionResult extends BTPLLMResult {
    /**
     * Completed text result
     */
    completion: string;
    /**
     * Completion stop reason
     */
    stop_reason: string;
}
/**
 * BTP LLM Anthropic Claude Client
 */
declare class BTPAnthropicClaudeClient extends BaseLLMClient {
    /**
     * @param httpClient HTTP Client
     */
    constructor(httpClient?: BTPLLMHttpClient);
    /**
     * Creates a Text Completion
     *
     * @param params the payload to send to the API
     * @param requestConfig Axios Request Configuration
     *
     * @see https://axios-http.com/docs/req_config
     * @see https://axios-http.com/docs/cancellation
     *
     * @returns HTTP Response data as JSON
     * @throws {@link BTPLLMError} in case of HTTP errors
     */
    createTextCompletion(params: BTPAnthropicClaudeTextParameters, requestConfig?: AxiosRequestConfig): Promise<BTPAnthropicClaudeCompletionResult>;
    /**
     * Creates a Chat Completion
     *
     * **Note**: Chat completion is just a wrapper around text completion with Chat-specific types.
     *
     * @param params the payload to send to the API
     * @param requestConfig Axios Request Configuration
     *
     * @see https://axios-http.com/docs/req_config
     * @see https://axios-http.com/docs/cancellation
     *
     * @returns HTTP Response data as JSON
     * @throws {@link BTPLLMError} in case of HTTP errors
     */
    createChatCompletion(params: BTPAnthropicClaudeChatParameters, requestConfig?: AxiosRequestConfig): Promise<BTPAnthropicClaudeCompletionResult>;
}

export { ANTHROPIC_AIPROMPT, ANTHROPIC_HUMANPROMPT, type BTPAnthropicClaudeChatMessage, type BTPAnthropicClaudeChatModel, type BTPAnthropicClaudeChatParameters, BTPAnthropicClaudeClient, type BTPAnthropicClaudeCompletionResult, type BTPAnthropicClaudeTextParameters };
