import { AxiosRequestConfig } from 'axios';
import { B as BTPBaseLLMParameters, a as BTPLLMResult, b as BaseLLMClient, c as BTPLLMHttpClient } from '../base-cac7307b.js';
import '../config-822cfdeb.js';
import '@sap-cloud-sdk/util';

/**
 * BTP AlephAlpha Luminous Text Models
 */
type BTPAlephAlphaLuminousTextModel = "alephalpha";
/**
 * BTP LLM AlephAlpha Luminous Text Completion Input Parameters
 */
interface BTPAlephAlphaLuminousTextCompletionParameters extends BTPBaseLLMParameters<BTPAlephAlphaLuminousTextModel> {
    /**
     * Input text string to complete
     */
    prompt: string;
    /**
     * The maximum number of tokens to be generated. Completion will terminate after the maximum number of tokens is reached.
     *
     * Increase this value to generate longer texts. A text is split into tokens. Usually there are more tokens than words.
     * The sum of input tokens and maximum_tokens may not exceed 2048.
     */
    maximum_tokens: number;
    /**
     * Generate at least this number of tokens before an end-of-text token is generated.
     *
     * @default 0
     */
    minimum_tokens?: number;
    /**
     * Echo the prompt in the completion. This may be especially helpful when log_probs is set to return logprobs for the prompt.
     *
     * @default false
     */
    echo?: boolean;
    /**
     * A higher sampling temperature encourages the model to produce less probable outputs ("be more creative").
     *
     * Values are expected in a range from 0.0 to 1.0. Try high values (e.g., 0.9) for a more "creative" response and
     * the default 0.0 for a well defined and repeatable answer.
     *
     * It is advised to use either temperature, `top_k`, or `top_p`, but not all three at the same time. If a combination
     * of `temperature`, `top_k` or `top_p` is used, rescaling of logits with temperature will be performed first. Then
     * `top_k` is applied, `top_p` follows last.
     *
     * @default 0
     */
    temperature?: number;
    /**
     * Introduces random sampling for generated tokens by randomly selecting the next token from the k most likely options.
     * A value larger than 1 encourages the model to be more creative. Set to 0.0 if repeatable output is desired. It is
     * advised to use either `temperature`, `top_k`, or `top_p`, but not all three at the same time. If a combination of
     * temperature, `top_k` or `top_p` is used, rescaling of logits with temperature will be performed first. Then `top_k`
     * is applied, `top_p` follows last.
     *
     * @default 0
     */
    top_k?: number;
    /**
     * Introduces random sampling for generated tokens by randomly selecting the next token from the smallest possible set of
     * tokens whose cumulative probability exceeds the probability `top_p`. Set to 0.0 if repeatable output is desired. It is
     * advised to use either `temperature`, `top_k`, or `top_p`, but not all three at the same time. If a combination of
     * `temperature`, `top_k` or `top_p` is used, rescaling of logits with temperature will be performed first. Then `top_k`
     * is applied, `top_p` follows last.
     */
    top_p?: number;
    /**
     * The presence penalty reduces the likelihood of generating tokens that are already present in the generated text
     * (`repetition_penalties_include_completion=true`) respectively the prompt (`repetition_penalties_include_prompt=true`).
     * Presence penalty is independent of the number of occurences. Increase the value to reduce the likelihood of repeating text.
     * An operation like the following is applied:
     *
     * ```
     * logits[t] -> logits[t] - 1 * penalty
     * ```
     *
     * where `logits[t]` is the logits for any given token. Note that the formula is independent of the number of times that a
     * token appears.
     *
     * @default 0
     */
    presence_penalty?: number;
    /**
     * The frequency penalty reduces the likelihood of generating tokens that are already present in the generated text
     * (`repetition_penalties_include_completion=true`) respectively the prompt (`repetition_penalties_include_prompt=true`).
     * If `repetition_penalties_include_prompt=True`, this also includes the tokens in the prompt. Frequency penalty is dependent
     * on the number of occurences of a token. An operation like the following is applied:
     *
     * ```
     * logits[t] -> logits[t] - count[t] * penalty
     * ```
     *
     * where `logits[t]` is the logits for any given token and `count[t]` is the number of times that token appears.
     *
     * @default 0
     */
    frequency_penalty?: number;
    /**
     * Increasing the sequence penalty reduces the likelihood of reproducing token sequences that already appear in the prompt
     * (if `repetition_penalties_include_prompt` is True) and prior completion.
     *
     * @default 0
     */
    sequence_penalty?: number;
    /**
     * Minimal number of tokens to be considered as sequence
     *
     * @default 2
     */
    sequence_penalty_min_length?: number;
    /**
     * Flag deciding whether presence penalty or frequency penalty are updated from tokens in the prompt
     *
     * @default false
     */
    repetition_penalties_include_prompt?: boolean;
    /**
     * Flag deciding whether presence penalty or frequency penalty are updated from tokens in the completion
     *
     * @default true
     */
    repetition_penalties_include_completion?: boolean;
    /**
     * Flag deciding whether presence penalty is applied multiplicatively (True) or additively (False). This changes the formula
     * stated for presence penalty.
     *
     * @default false
     */
    use_multiplicative_presence_penalty?: boolean;
    /**
     * Flag deciding whether frequency penalty is applied multiplicatively (True) or additively (False). This changes the formula
     * stated for frequency penalty.
     *
     * @default false
     */
    use_multiplicative_frequency_penalty?: boolean;
    /**
     * Flag deciding whether sequence penalty is applied multiplicatively (True) or additively (False).
     *
     * @default false
     */
    use_multiplicative_sequence_penalty?: boolean;
    /**
     * All tokens in this text will be used in addition to the already penalized tokens for repetition penalties. These consist
     * of the already generated completion tokens and the prompt tokens, if `repetition_penalties_include_prompt` is set to `true`.
     */
    penalty_bias?: string;
    /**
     * List of strings that may be generated without penalty, regardless of other penalty settings. By default, we will also include
     * any `stop_sequences` you have set, since completion performance can be degraded if expected stop sequences are penalized.
     * You can disable this behavior by setting `penalty_exceptions_include_stop_sequences` to `false`.
     */
    penalty_exceptions?: string[];
    /**
     * By default we include all `stop_sequences` in `penalty_exceptions`, so as not to penalise the presence of stop sequences that
     * are present in few-shot prompts to give structure to your completions.
     *
     * You can set this to false if you do not want this behaviour.
     *
     * See the description of `penalty_exceptions` for more information on what `penalty_exceptions` are used for.
     *
     * @default true
     */
    penalty_exceptions_include_stop_sequences?: boolean;
    /**
     * If a value is given, the number of `best_of` completions will be generated on the server side. The completion with the
     * highest log probability per token is returned. If the parameter `n` is greater than 1 more than 1 (`n`) completions will
     * be returned. best_of must be strictly greater than `n`.
     *
     * @default 1
     */
    best_of?: number;
    /**
     * The number of completions to return. If argmax sampling is used (`temperature`, `top_k`, `top_p` are all default)
     * the same completions will be produced. This parameter should only be increased if random sampling is used.
     *
     * @default 1
     */
    n?: number;
    /**
     * Logit bias
     */
    logit_bias?: object;
    /**
     * Number of top log probabilities for each token generated. Log probabilities can be used in downstream tasks or
     * to assess the model's certainty when producing tokens. No log probabilities are returned if set to None. Log
     * probabilites of generated tokens are returned if set to 0. Log probabilities of generated tokens and top `n`
     * log probabilites are returned if set to `n`.
     */
    log_probs?: number;
    /**
     * List of strings that will stop generation if they're generated. Stop sequences may be helpful in structured texts.
     */
    stop_sequences?: string[];
    /**
     * Flag indicating whether individual tokens of the completion should be returned (True) or whether solely the
     *  generated text (i.e. the completion) is sufficient (False).
     *
     * @default false
     */
    tokens?: boolean;
    /**
     * Setting this parameter to true forces the raw completion of the model to be returned. For some models,
     * we may optimize the completion that was generated by the model and return the optimized completion in
     * the completion field of the CompletionResponse. The raw completion, if returned, will contain the
     * un-optimized completion. Setting tokens to true or log_probs to any value will also trigger the raw
     * completion to be returned.
     *
     * @default false
     */
    raw_completion?: boolean;
    /**
     * We continually research optimal ways to work with our models. By default, we apply these optimizations
     * to both your prompt and completion for you. Our goal is to improve your results while using our API.
     * But you can always pass disable_optimizations: true and we will leave your prompt and completion untouched.
     *
     * @default false
     */
    disable_optimizations?: boolean;
    /**
     * Bias the completion to only generate options within this list; all other tokens are disregarded at sampling
     *
     * Note that strings in the inclusion list must not be prefixes of strings in the exclusion list and vice versa
     */
    completion_bias_inclusion?: string[];
    /**
     * Only consider the first token for the completion_bias_inclusion
     *
     * @default false
     */
    completion_bias_inclusion_first_token_only?: boolean;
    /**
     * Bias the completion to NOT generate options within this list; all other tokens are unaffected in sampling
     *
     * Note that strings in the inclusion list must not be prefixes of strings in the exclusion list and vice versa
     *
     * @default []
     */
    completion_bias_exclusion?: string[];
    /**
     * Only consider the first token for the completion_bias_exclusion
     *
     * @default false
     */
    completion_bias_exclusion_first_token_only?: boolean;
    /**
     * If set to `null`, attention control parameters only apply to those tokens that have explicitly been set
     * in the request. If set to a non-null value, we apply the control parameters to similar tokens as well.
     * Controls that have been applied to one token will then be applied to all other tokens that have at least
     * the similarity score defined by this parameter. The similarity score is the cosine similarity of token embeddings.
     */
    contextual_control_threshold?: number;
    /**
     * `true`: apply controls on prompt items by adding the `log(control_factor)` to attention scores
     *
     * `false`: apply controls on prompt items by `(attention_scores - -attention_scores.min(-1)) * control_factor`
     *
     * @default true
     */
    control_log_additive?: boolean;
}
/**
 * BTP LLM AlephAlpha Luminous Completion Output Result
 */
interface BTPAlephAlphaLuminousCompletionResult extends BTPLLMResult {
    /**
     * Model name and version (if any) of the used model for inference
     */
    model_version: string;
    /**
     * Describes prompt after optimizations. This field is only returned if the flag `disable_optimizations` flag
     * is not set and the prompt has actually changed.
     */
    optimized_prompt: {
        /**
         * Type of prompt
         */
        type: string;
        /**
         * Prompt text
         */
        data: string;
    }[];
    /**
     * List of completions; may contain only one entry if no more are requested (see parameter `n`)
     */
    completions: {
        /**
         * list with a dictionary for each generated token. The dictionary maps the keys' tokens to the respective
         * log probabilities. This field is only returned if requested with the parameter `log_probs`.
         */
        log_probs?: object;
        /**
         * Generated completion on the basis of the prompt
         */
        completion: string;
        /**
         * For some models, we may optimize the completion that was generated by the model and return the optimized
         * completion in the completion field of the CompletionResponse. The raw completion, if returned, will contain
         * the un-optimized completion. Setting the parameter `raw_completion` in the CompletionRequest to true forces
         * the raw completion of the model to be returned. Setting tokens to true or `log_probs` to any value will also
         * trigger the raw completion to be returned.
         */
        raw_completion?: string;
        /**
         * Completion split into tokens. This field is only returned if requested with the parameter `tokens`.
         */
        completion_tokens?: string[];
        /**
         * Reason for termination of generation. This may be a stop sequence or maximum number of tokens reached.
         */
        finish_reason: "maximum_tokens";
    }[];
}
/**
 * BTP LLM AlephAlpha Luminous Client
 */
declare class BTPAlephAlphaLuminousClient extends BaseLLMClient {
    /**
     * @param httpClient HTTP Client
     */
    constructor(httpClient?: BTPLLMHttpClient);
    /**
     * Creates a Text Completion
     *
     * @param params the payload to send to the API
     * @param requestConfig Axios Request Configuration
     *
     * @see https://axios-http.com/docs/req_config
     * @see https://axios-http.com/docs/cancellation
     *
     * @returns HTTP Response data as JSON
     * @throws {@link BTPLLMError} in case of HTTP errors
     */
    createTextCompletion(params: BTPAlephAlphaLuminousTextCompletionParameters, requestConfig?: AxiosRequestConfig): Promise<BTPAlephAlphaLuminousCompletionResult>;
}

export { BTPAlephAlphaLuminousClient, type BTPAlephAlphaLuminousCompletionResult, type BTPAlephAlphaLuminousTextCompletionParameters, type BTPAlephAlphaLuminousTextModel };
