import { AxiosRequestConfig } from 'axios';
import { B as BTPBaseLLMParameters, a as BTPLLMResult, b as BaseLLMClient, c as BTPLLMHttpClient } from '../base-cac7307b.js';
import '../config-822cfdeb.js';
import '@sap-cloud-sdk/util';

/**
 * BTP AI21 Jurassic Text Models
 */
type BTPAI21JurassicTextModel = "ai21-j2-grande-instruct" | "ai21-j2-jumbo-instruct";
/**
 * BTP AI21 Jurassic Penalty Parameters
 */
interface BTPAI21JurassicPenaltyParameters {
    /**
     * Controls the magnitude of the penalty. A positive penalty value implies reducing the probability of repetition. Larger values correspond
     * to a stronger bias against repetition.
     *
     * Allowed Values: 1 <= int <= 500
     */
    scale: number;
    /**
     * Determines whether the penalty is applied to purely-numeric tokens, such as `2022` or `123`. Tokens that contain numbers and letters,
     * such as `20th`, are not affected by this parameter.
     *
     * @default true
     */
    applyToNumbers?: boolean;
    /**
     * Determines whether the penalty is applied to tokens containing punctuation characters and whitespaces, such as `;` `,` `!!!` or `▁\\[[@`.
     *
     * @default true
     */
    applyToPunctuations?: boolean;
    /**
     * Determines whether the penalty is applied to tokens that are NLTK English stopwords or multi-word combinations of these words, such as
     * `are` , `nor` and `▁We▁have`.
     *
     * @default true
     */
    applyToStopwords?: boolean;
    /**
     * Apply the penalty to whitespaces and newlines.
     *
     * @default true
     */
    applyToWhitespaces?: boolean;
    /**
     * Determines whether the penalty is applied to any of approximately 650 common emojis in the Jurassic-1 vocabulary.
     *
     * @default true
     */
    applyToEmojis?: boolean;
}
/**
 * BTP LLM AI21 Jurassic Completion Parameters
 */
interface BTPAI21JurassicCompletionParameters extends BTPBaseLLMParameters<BTPAI21JurassicTextModel> {
    /**
     * This is the starting point for generating responses. The format of the prompt, whether zero-shot,
     * few-shot, or instructional, can influence the shape of the model's responses. Text for model to continue
     * should not exceed 8191 tokens
     */
    prompt: string;
    /**
     * The number of responses to generate for a given prompt. A value greater than 1 is meaningful only in case
     * of non-greedy decoding, i.e. temperature > 0.
     *
     * @default 1
     */
    numResults?: number;
    /**
     * The maximum number of tokens to generate for each response. If no `stopSequences` are given, generation is
     * stopped after producing `maxTokens`.
     *
     * @default 16
     */
    maxTokens?: number;
    /**
     * The minimum number of tokens to generate for each response. If `stopSequences` are given, they are ignored
     * until `minTokens` are generated.
     *
     * @default 0
     */
    minTokens?: number;
    /**
     * A value controlling the "creativity" of the model's responses. Modifies the distribution from which tokens are sampled.
     *
     * Setting temperature to 1.0 samples directly from the model distribution. Lower (higher) values increase the chance of
     * sampling higher (lower) probability tokens. A value of 0 essentially disables sampling and results in greedy decoding,
     * where the most likely token is chosen at every step.
     *
     * @default 0.7
     */
    temperature?: number;
    /**
     * A value controlling the diversity of the model's responses. Sample tokens from the corresponding top percentile of
     * probability mass.
     *
     * For example, a value of 0.9 will only consider tokens comprising the top 90% probability mass.
     *
     * @default 1
     *
     */
    topP?: number;
    /**
     * The number of top-scoring tokens to consider for each generation step.
     *
     * Return the top-K alternative tokens. When using a non-zero value, the response includes the string representations
     * and logprobs for each of the top-K alternatives at each position, in the prompt and in the completions.
     *
     * @default 0
     */
    topKReturn?: number;
    /**
     * A list of sequences that, when generated, will cause the model to stop generating tokens.
     *
     * For example, to stop at a comma or a new line use `[".", "\n"]`. The decoded result text will not include the stop sequence
     * string, but it will be included in the raw token data, which can also continue beyond the stop sequence if the sequence ended
     * in the middle of a token. The sequence which triggered the termination will be included in the finishReason of the response.
     *
     */
    stopSequences?: string[];
    /**
     * A penalty applied to tokens that are frequently generated.
     */
    frequencyPenalty?: BTPAI21JurassicPenaltyParameters;
    /**
     * A penalty applied to tokens based on their frequency in the generated responses.
     */
    countPenalty?: BTPAI21JurassicPenaltyParameters;
    /**
     * A penalty applied to tokens that are already present in the prompt.
     */
    presencePenalty?: BTPAI21JurassicPenaltyParameters;
}
/**
 * BTP AI21 Jurassic Completion Token Info
 */
interface BTPAI21JurassicCompletionTokenInfo {
    /**
     * The TokenData object provides detailed information about each token in both the prompt and the completions.
     */
    generatedToken: {
        /**
         * The string representation of the token.
         */
        token: string;
        /**
         * The predicted log probability of the token as a float value.
         */
        logprob: number;
        /**
         * The raw predicted log probability of the token as a float value.
         */
        raw_logprob: number;
    };
    /**
     * The topTokens field is a list of the top K alternative tokens for this position, sorted by probability,
     * according to the `topKReturn` request parameter. If `topKReturn` is set to 0, this field will be `null`.
     */
    topTokens: {
        /**
         * The string representation of the token.
         */
        token: string;
        /**
         * The predicted log probability of the token as a float value.
         */
        logprob: number;
    }[] | null;
    /**
     * The `textRange` field indicates the start and end offsets of the token in the decoded text string
     */
    textRange: {
        /**
         * The starting index of the token in the decoded text string.
         */
        start: number;
        /**
         * The ending index of the token in the decoded text string.
         */
        end: number;
    };
}
/**
 * BTP LLM AI21 Jurassic Text Completion Output Result
 */
interface BTPAI21JurassicTextCompletionResult extends BTPLLMResult {
    /**
     * A unique string id for the processed request. Repeated identical requests receive different IDs.
     */
    id: string;
    /**
     * The prompt includes the raw text, the tokens with their log probabilities, and the top-K alternative tokens at
     * each position, if requested.
     */
    prompt: {
        /**
         * Input text prompt
         */
        text: string;
        /**
         * Input text tokens
         */
        tokens: BTPAI21JurassicCompletionTokenInfo[];
    };
    /**
     * A list of completions, including raw text, tokens, and log probabilities. The number of completions corresponds
     * to the requested `numResults`.
     */
    completions: {
        /**
         * contains the `text` (string) and tokens (list of TokenData) for the completion.
         */
        data: {
            /**
             * The generated text completion.
             */
            text: string;
            /**
             * Output generated text tokens
             */
            tokens: BTPAI21JurassicCompletionTokenInfo[];
        };
        /**
         * a nested data structure describing the reason generation was terminated for this completion.
         */
        finishReason: {
            /**
             * Finish reason
             */
            reason: string;
            /**
             * Length of completed text tokens
             */
            length: number;
        };
    }[];
}
/**
 * BTP LLM AI21 Jurassic Client
 */
declare class BTPAI21JurassicClient extends BaseLLMClient {
    /**
     * @param httpClient HTTP Client
     */
    constructor(httpClient?: BTPLLMHttpClient);
    /**
     * Creates a Text Completion
     *
     * @param params the payload to send to the API
     * @param requestConfig Axios Request Configuration
     *
     * @see https://axios-http.com/docs/req_config
     * @see https://axios-http.com/docs/cancellation
     *
     * @returns HTTP Response data as JSON
     * @throws {@link BTPLLMError} in case of HTTP errors
     */
    createTextCompletion(params: BTPAI21JurassicCompletionParameters, requestConfig?: AxiosRequestConfig): Promise<BTPAI21JurassicTextCompletionResult>;
}

export { BTPAI21JurassicClient, type BTPAI21JurassicCompletionParameters, type BTPAI21JurassicCompletionTokenInfo, type BTPAI21JurassicPenaltyParameters, type BTPAI21JurassicTextCompletionResult, type BTPAI21JurassicTextModel };
