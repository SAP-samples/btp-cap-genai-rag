import { AxiosRequestConfig } from 'axios';
import { B as BTPBaseLLMParameters, a as BTPLLMResult, b as BaseLLMClient, c as BTPLLMHttpClient } from '../base-cac7307b.js';
import '../config-822cfdeb.js';
import '@sap-cloud-sdk/util';

/**
 * BTP HuggingFace Text Models
 */
type BTPHuggingFaceTextModel = "falcon-7b" | "falcon-40b-instruct" | "llama2-13b-chat-hf" | "llama2-70b-chat-hf";
/**
 * Base BTP LLM HuggingFace Model Completion Parameters
 */
interface BTPHuggingFaceParameters {
}
/**
 * BTP LLM HuggingFace Model Text Completion Input Parameters
 */
interface BTPHuggingFaceTextCompletionParameters extends BTPBaseLLMParameters<BTPHuggingFaceTextModel>, BTPHuggingFaceParameters {
    /**
     * A string to be generated from
     */
    inputs: string;
    /**
     * Model parameters
     */
    parameters?: {
        /**
         * Whether or not to use sampling, use greedy decoding otherwise.
         */
        do_sample?: boolean;
        /**
         * The amount of new tokens to be generated, this does not include the input length it is a estimate of
         * the size of generated text you want. Each new tokens slows down the request, so look for balance between
         * response times and length of text generated.
         *
         * Allowed values: Int (0 - 250)
         */
        max_new_tokens?: number;
        /**
         * The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will
         * be a soft limit. Use that in combination with max_new_tokens for best results.
         *
         * Allowed values: Float (0-120.0)
         */
        max_time?: number;
        /**
         * The number of proposition you want to be returned.
         *
         * @default 1
         */
        num_return_sequences?: number;
        /**
         * The more a token is used within generation the more it is penalized to not be picked in successive generation passes.
         *
         * Allowed values: Float (0.0-100.0)
         */
        repetition_penalty?: number;
        /**
         * If set to False, the return results will not contain the original query making it easier for prompting.
         *
         * @default true
         */
        return_full_text?: boolean;
        /**
         * The temperature of the sampling operation. 1 means regular sampling, 0 means always take the highest score,
         * 100.0 is getting closer to uniform probability.
         *
         * Allowed values: Float (0.0-100.0)
         *
         * @default 1.0
         */
        temperature?: number;
        /**
         * Integer to define the top tokens considered within the sample operation to create new text.
         */
        top_k?: number;
        /**
         * Float to define the tokens that are within the sample operation of text generation. Add tokens in the sample for
         * more probable to least probable until the sum of the probabilities is greater than `top_p`.
         */
        top_p?: number;
        /**
         * Stop generating tokens if a member of `stop_sequences` is generated
         */
        stop?: string[];
        /**
         * The maximum number of tokens from the input.
         */
        truncate?: number;
    };
}
/**
 * BTP LLM HuggingFace Model Text Completion Output Result
 */
interface BTPHuggingFaceTextCompletionResult extends BTPLLMResult {
    /**
     * The continuated string
     */
    generated_text: string;
}
/**
 * BTP LLM HuggingFace Model Client
 */
declare class BTPHuggingFaceClient extends BaseLLMClient {
    /**
     * @param httpClient HTTP Client
     */
    constructor(httpClient?: BTPLLMHttpClient);
    /**
     * Creates a Text Completion
     *
     * @param params the payload to send to the API
     * @param requestConfig Axios Request Configuration
     *
     * @see https://axios-http.com/docs/req_config
     * @see https://axios-http.com/docs/cancellation
     *
     * @returns HTTP Response data as JSON
     * @throws {@link BTPLLMError} in case of HTTP errors
     */
    textGeneration(params: BTPHuggingFaceTextCompletionParameters, requestConfig?: AxiosRequestConfig): Promise<BTPHuggingFaceTextCompletionResult>;
}

export { BTPHuggingFaceClient, type BTPHuggingFaceTextCompletionParameters, type BTPHuggingFaceTextCompletionResult, type BTPHuggingFaceTextModel };
