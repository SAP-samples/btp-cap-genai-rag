import { AxiosRequestConfig } from 'axios';
import { B as BTPBaseLLMParameters, a as BTPLLMResult, b as BaseLLMClient, c as BTPLLMHttpClient } from '../base-cac7307b.js';
import '../config-822cfdeb.js';
import '@sap-cloud-sdk/util';

/**
 * BTP Google PaLM Text Models
 */
type BTPGooglePaLMTextModel = "gcp-text-bison-001";
/**
 * BTP Google PaLM Chat Models
 */
type BTPGooglePaLMChatModel = "gcp-chat-bison-001";
/**
 * BTP Google PaLM Embedding Models
 */
type BTPGooglePaLMEmbeddingModel = "gcp-textembedding-gecko-001";
/**
 * Google PaLM Text Parameter Instance
 */
type BTPGooglePaLMTextParamInstance = {
    /**
     * A prompt is a natural language request submitted to a language model to receive a response back.
     * Prompts can contain questions, instructions, contextual information, examples, and text for the
     * model to complete or continue.
     */
    prompt: string;
};
/**
 * Google PaLM Chat Parameter Instance
 */
type BTPGooglePaLMChatParamInstance = {
    /**
     * Context can be instructions that you give to the model on how it should respond or information
     * that it uses or references to generate a response. Add contextual information in your prompt
     * when you need to give information to the model, or restrict the boundaries of the responses
     * to only what's within the context.
     */
    context?: string;
    /**
     * Examples are a list of structured messages to the model to learn how to respond to the conversation.
     */
    examples?: BTPGooglePaLMChatExample[];
    /**
     * Conversation history provided to the model in a structured alternate-author form. Messages appear
     * in chronological order: oldest first, newest last. When the history of messages causes the input
     * to exceed the maximum length, the oldest messages are removed until the entire prompt is within
     * the allowed limit.
     *
     * **Note**: There must be an odd number of messages (AUTHOR-CONTENT pairs) for the model to generate a response.
     */
    messages: BTPGooglePaLMChatMessage[];
};
/**
 * Google PaLM Embedding Parameter Instance
 */
type BTPGooglePaLMEmbeddingParamInstance = {
    /**
     *  The text that you want to generate embeddings for
     */
    content: string;
};
/**
 * Google PaLM Chat Example
 */
type BTPGooglePaLMChatExample = {
    /**
     * Example of a message
     */
    input: {
        content: string;
    };
    /**
     * Example of the ideal response.
     */
    output: {
        content: string;
    };
};
/**
 * Google PaLM Chat Message
 */
type BTPGooglePaLMChatMessage = {
    /**
     * The author of the message
     */
    author: string;
    /**
     * The content of the message
     */
    content: string;
};
/**
 * Base BTP LLM Google Palm Prediction Parameters
 */
interface BTPGooglePaLMParameters<T> {
    /**
     * Input parameter instances
     */
    instances: T[];
    /**
     * LLM Parameters
     */
    parameters: {
        /**
         * The temperature is used for sampling during response generation, which occurs when `topP` and `topK` are applied.
         * Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that
         * require a more deterministic and less open-ended or creative response, while higher temperatures can lead to more
         * diverse or creative results.
         *
         * A temperature of 0 is deterministic, meaning that the highest probability response is always selected.
         *
         * For most use cases, try starting with a temperature of 0.2. If the model returns a response that's too generic,
         * too short, or the model gives a fallback response, try increasing the temperature.
         */
        temperature?: number;
        /**
         * Maximum number of tokens that can be generated in the response. A token is approximately four characters.
         * 100 tokens correspond to roughly 60-80 words.
         *
         * Specify a lower value for shorter responses and a higher value for longer responses.
         */
        maxOutputTokens: number;
        /**
         * Top-P changes how the model selects tokens for output. Tokens are selected from the most (see top-K) to least probable
         *  until the sum of their probabilities equals the top-P value.
         *
         * For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the top-P value is 0.5, then the model
         * will select either A or B as the next token by using temperature and excludes C as a candidate.
         *
         * Specify a lower value for less random responses and a higher value for more random responses.
         *
         * @default 0.95.
         */
        topP?: number;
        /**
         * Top-K changes how the model selects tokens for output. A top-K of 1 means the next selected token is the most probable
         * among all tokens in the model's vocabulary (also called greedy decoding), while a top-K of 3 means that the next token
         * is selected from among the three most probable tokens by using temperature.
         *
         * For each token selection step, the top-K tokens with the highest probabilities are sampled. Then tokens are further
         * filtered based on top-P with the final token selected using temperature sampling.
         *
         * Specify a lower value for less random responses and a higher value for more random responses.
         *
         * @default 40
         */
        topK?: number;
    };
}
/**
 * BTP LLM Google PaLM Text Prediction Input Parameters
 */
interface BTPGooglePaLMTextPredictionParameters extends BTPBaseLLMParameters<BTPGooglePaLMTextModel>, BTPGooglePaLMParameters<BTPGooglePaLMTextParamInstance> {
}
/**
 * BTP LLM Google PaLM Chat Prediction Input Parameters
 */
interface BTPGooglePaLMChatPredictionParameters extends BTPBaseLLMParameters<BTPGooglePaLMChatModel>, BTPGooglePaLMParameters<BTPGooglePaLMChatParamInstance> {
}
/**
 * BTP LLM Google PaLM Embedding Input Parameters
 */
interface BTPGooglePaLMEmbeddingParameters extends BTPBaseLLMParameters<BTPGooglePaLMEmbeddingModel>, Omit<BTPGooglePaLMParameters<BTPGooglePaLMEmbeddingParamInstance>, "parameters"> {
}
/**
 * BTP Google PaLM Citation Metadata result
 */
interface BTPGooglePaLMCitationMetadata {
    /**
     * List of citations
     */
    citations: {
        /**
         * Start of the citation index
         */
        startIndex?: number;
        /**
         * End of the citation index
         */
        endIndex?: number;
        /**
         * Citation source
         */
        uri?: string;
        /**
         * Citation license
         */
        license?: string;
    }[];
}
/**
 * BTP Google PaLM Safety Attributes result
 */
interface BTPGooglePaLMSafetyAttributes {
    /**
     * Blocked based on configuration
     */
    blocked: boolean;
    /**
     * Safety categories
     */
    categories: string[];
    /**
     * Confidence score for each of the classified categories
     */
    scores: number[];
}
/**
 * Base BTP LLM Google PaLM Prediction Output Result
 */
interface BTPGooglePaLMLLMOutput extends BTPLLMResult {
    /**
     * Output metadata
     */
    metadata: {
        tokenMetadata: {
            /**
             * Input token count
             */
            inputTokenCount: {
                /**
                 * Total number of input text characters
                 */
                totalBillableCharacters: number;
                /**
                 * Total number of input tokens
                 *
                 * **Note**: A token is approximately four characters. 100 tokens correspond to roughly 60-80 words.
                 */
                totalTokens: number;
            };
            /**
             * Output token count
             */
            outputTokenCount: {
                /**
                 * Total number of output text characters
                 */
                totalBillableCharacters: number;
                /**
                 * Total number of output tokens
                 *
                 * **Note**: A token is approximately four characters. 100 tokens correspond to roughly 60-80 words.
                 */
                totalTokens: number;
            };
        };
    };
}
/**
 * BTP LLM Google PaLM Text Prediction Output Result
 */
interface BTPGooglePaLMTextPredictionResult extends BTPGooglePaLMLLMOutput {
    /**
     * Prediction results
     */
    predictions: {
        /**
         * Predicted text output
         */
        content: string;
        /**
         * Citation information
         */
        citationMetadata: BTPGooglePaLMCitationMetadata;
        /**
         * Safety attributes
         */
        safetyAttributes: BTPGooglePaLMSafetyAttributes;
    }[];
}
/**
 * BTP LLM Google PaLM Chat Prediction Output Result
 */
interface BTPGooglePaLMChatPredictionResult extends BTPGooglePaLMLLMOutput {
    /**
     * Prediction results
     */
    predictions: {
        candidates: BTPGooglePaLMChatMessage[];
        /**
         * Citation information
         */
        citationMetadata: BTPGooglePaLMCitationMetadata[];
        /**
         * Safety attributes
         */
        safetyAttributes: BTPGooglePaLMSafetyAttributes[];
    }[];
}
/**
 * BTP LLM Google PaLM Embedding Output Result
 */
interface BTPGooglePaLMEmbeddingResult extends BTPLLMResult {
    /**
     * Output metadata
     */
    metadata: {
        /**
         * Total number of input text characters
         */
        billableCharacterCount: number;
    };
    /**
     * Prediction results
     */
    predictions: {
        /**
         * Embeddings result
         */
        embeddings: {
            /**
             * Result statistics
             */
            statistics: {
                /**
                 * Total number of input tokens
                 *
                 * **Note**: A token is approximately four characters. 100 tokens correspond to roughly 60-80 words.
                 */
                token_count: number;
                /**
                 * Input text truncated
                 */
                truncated: boolean;
            };
            /**
             * Embeddings
             */
            values: number[];
        };
    }[];
}
/**
 * BTP LLM Google PaLM Client
 */
declare class BTPGooglePaLMClient extends BaseLLMClient {
    /**
     * @param httpClient HTTP Client
     */
    constructor(httpClient?: BTPLLMHttpClient);
    /**
     * Creates a Text Prediction
     *
     * @param params the payload to send to the API
     * @param requestConfig Axios Request Configuration
     *
     * @see https://axios-http.com/docs/req_config
     * @see https://axios-http.com/docs/cancellation
     *
     * @returns HTTP Response data as JSON
     * @throws {@link BTPLLMError} in case of HTTP errors
     */
    predictText(params: BTPGooglePaLMTextPredictionParameters, requestConfig?: AxiosRequestConfig): Promise<BTPGooglePaLMTextPredictionResult>;
    /**
     * Creates a Chat Prediction
     *
     * @param params the payload to send to the API
     * @param requestConfig Axios Request Configuration
     *
     * @see https://axios-http.com/docs/req_config
     * @see https://axios-http.com/docs/cancellation
     *
     * @returns HTTP Response data as JSON
     * @throws {@link BTPLLMError} in case of HTTP errors
     */
    predictChat(params: BTPGooglePaLMChatPredictionParameters, requestConfig?: AxiosRequestConfig): Promise<BTPGooglePaLMChatPredictionResult>;
    /**
     * Creates an Embedding
     *
     * @param params the payload to send to the API
     * @param requestConfig Axios Request Configuration
     *
     * @see https://axios-http.com/docs/req_config
     * @see https://axios-http.com/docs/cancellation
     *
     * @returns HTTP Response data as JSON
     * @throws {@link BTPLLMError} in case of HTTP errors
     */
    embedText(params: BTPGooglePaLMEmbeddingParameters, requestConfig?: AxiosRequestConfig): Promise<BTPGooglePaLMEmbeddingResult>;
}

export { type BTPGooglePaLMChatExample, type BTPGooglePaLMChatMessage, type BTPGooglePaLMChatModel, type BTPGooglePaLMChatParamInstance, type BTPGooglePaLMChatPredictionParameters, type BTPGooglePaLMChatPredictionResult, BTPGooglePaLMClient, type BTPGooglePaLMEmbeddingModel, type BTPGooglePaLMEmbeddingParamInstance, type BTPGooglePaLMEmbeddingParameters, type BTPGooglePaLMEmbeddingResult, type BTPGooglePaLMTextModel, type BTPGooglePaLMTextParamInstance, type BTPGooglePaLMTextPredictionParameters, type BTPGooglePaLMTextPredictionResult };
