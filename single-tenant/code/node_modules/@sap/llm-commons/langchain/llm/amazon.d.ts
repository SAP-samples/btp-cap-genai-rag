import { CallbackManagerForLLMRun } from 'langchain/callbacks';
import { BaseLLMParams, LLM } from 'langchain/llms/base';
import { BTPAmazonTitanTextModel } from '../../client/amazon.js';
import { B as BTPBaseLLMParameters } from '../../base-cac7307b.js';
import 'axios';
import '../../config-822cfdeb.js';
import '@sap-cloud-sdk/util';

/**
 * Input for Text generation for Amazon Titan
 */
interface BTPAmazonTitanInput extends BTPBaseLLMParameters<BTPAmazonTitanTextModel>, BaseLLMParams {
    /**
     * The maximum number of tokens to generate in the completion.
     */
    maxTokenCount?: number;
    /**
     * What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 for ones with a well-defined answer.
     */
    temperature?: number;
    /**
     * Stop sequences where the API will stop generating further tokens.
     */
    stopSequences?: unknown[];
    /**
     * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
     */
    topP?: number;
}
/**
 * AmazonTitan Language Model Wrapper to generate texts
 */
declare class BTPAmazonTitan extends LLM implements BTPAmazonTitanInput {
    private btpAmazonTitanClient;
    deployment_id: BTPAmazonTitanTextModel;
    maxTokenCount?: number;
    temperature?: number;
    stopSequences?: unknown[];
    topP?: number;
    constructor(fields?: Partial<BTPAmazonTitanInput>);
    _llmType(): string;
    _call(prompt: string, options: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): Promise<string>;
}

export { BTPAmazonTitan, type BTPAmazonTitanInput };
