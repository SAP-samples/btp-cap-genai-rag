'use strict';

var openai = require('langchain/chat_models/openai');
var schema = require('langchain/schema');
var zodToJsonSchema = require('zod-to-json-schema');
var client_btp = require('../../client/btp.cjs');
var index = require('../../index-2134ea38.cjs');
require('../../utils-b80ecc7f.cjs');
require('@sap-cloud-sdk/util');
require('../../client/ai21.cjs');
require('../../base-921f7eb6.cjs');
require('axios');
require('../../client/alephalpha.cjs');
require('../../client/amazon.cjs');
require('../../client/anthropic.cjs');
require('../../client/cohere.cjs');
require('../../client/google.cjs');
require('../../client/huggingface.cjs');
require('../../client/openai.cjs');
require('@sap/xsenv');
require('fs');
require('util');

var __defProp = Object.defineProperty;
var __defProps = Object.defineProperties;
var __getOwnPropDescs = Object.getOwnPropertyDescriptors;
var __getOwnPropSymbols = Object.getOwnPropertySymbols;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __propIsEnum = Object.prototype.propertyIsEnumerable;
var __defNormalProp = (obj, key, value) => key in obj ? __defProp(obj, key, { enumerable: true, configurable: true, writable: true, value }) : obj[key] = value;
var __spreadValues = (a, b) => {
  for (var prop in b || (b = {}))
    if (__hasOwnProp.call(b, prop))
      __defNormalProp(a, prop, b[prop]);
  if (__getOwnPropSymbols)
    for (var prop of __getOwnPropSymbols(b)) {
      if (__propIsEnum.call(b, prop))
        __defNormalProp(a, prop, b[prop]);
    }
  return a;
};
var __spreadProps = (a, b) => __defProps(a, __getOwnPropDescs(b));
var __publicField = (obj, key, value) => {
  __defNormalProp(obj, typeof key !== "symbol" ? key + "" : key, value);
  return value;
};
class BTPLLMProxyChat extends openai.ChatOpenAI {
  constructor(fields) {
    var _a, _b, _c, _d, _e, _f, _g;
    super(__spreadProps(__spreadValues({}, fields), { openAIApiKey: "dummy" }));
    __publicField(this, "btpLLMProxyClient");
    __publicField(this, "deployment_id");
    __publicField(this, "maxOutputTokens");
    __publicField(this, "topK");
    this.deployment_id = (_a = fields == null ? void 0 : fields.deployment_id) != null ? _a : "gpt-35-turbo";
    this.temperature = (_b = fields == null ? void 0 : fields.temperature) != null ? _b : this.temperature;
    this.maxOutputTokens = (_c = fields == null ? void 0 : fields.maxOutputTokens) != null ? _c : this.maxOutputTokens;
    this.topP = (_d = fields == null ? void 0 : fields.topP) != null ? _d : this.topP;
    this.topK = (_e = fields == null ? void 0 : fields.topK) != null ? _e : this.topK;
    this.n = (_f = fields == null ? void 0 : fields.n) != null ? _f : 1;
    this.stop = (_g = fields == null ? void 0 : fields.stop) != null ? _g : this.stop;
    this.btpLLMProxyClient = new client_btp.BTPLLMProxyClient();
  }
  get callKeys() {
    return [...super.callKeys, "options", "function_call", "functions", "tools"];
  }
  get lc_secrets() {
    return {};
  }
  get lc_aliases() {
    return {};
  }
  _llmType() {
    return "btp-llm-proxy";
  }
  async _generate(messages, options, runManager) {
    const res = await this.caller.callWithOptions(
      {
        signal: options.signal
      },
      () => {
        var _a, _b;
        return this.btpLLMProxyClient.createChatCompletion(
          __spreadValues({
            messages: this.mapBaseMessagesToBTPLLMProxyMessages(messages, options),
            deployment_id: this.deployment_id,
            maxOutputTokens: this.maxOutputTokens === -1 ? void 0 : this.maxOutputTokens,
            temperature: this.temperature,
            topP: this.topP,
            topK: this.topK,
            n: this.n,
            stop: (_a = options == null ? void 0 : options.stop) != null ? _a : this.stop,
            functions: (_b = options == null ? void 0 : options.functions) != null ? _b : (options == null ? void 0 : options.tools) ? options == null ? void 0 : options.tools.map(this.mapToolToBTPLLMProxyFunction.bind(this)) : void 0
          }, this.modelKwargs),
          {
            signal: options.signal,
            timeout: this.timeout || options.timeout
          }
        );
      }
    );
    await (runManager == null ? void 0 : runManager.handleLLMNewToken(res.choices[0].message.content || ""));
    return this.mapBTPLLMProxyMessagesToChatResult(res);
  }
  /**
   * Maps a LangChain {@link StructuredTool} to {@link BTPLLMProxyFunction}
   */
  mapToolToBTPLLMProxyFunction(tool) {
    return {
      name: tool.name,
      description: tool.description,
      parameters: zodToJsonSchema.zodToJsonSchema(tool.schema)
    };
  }
  /**
   * Maps a {@link BaseMessage} to BTP LLM Proxy's Message Role
   */
  mapBaseMessageToRole(message) {
    switch (message._getType()) {
      case "ai":
        return "assistant";
      case "human":
        return "user";
      case "system":
        return "system";
      case "function":
        return "assistant";
      case "generic":
        return message.role;
      default:
        throw new index.BTPLLMError(`Unknown message type: ${message._getType()}`);
    }
  }
  /**
   * Maps {@link BaseMessage} to BTP LLM Proxy Messages
   */
  mapBaseMessagesToBTPLLMProxyMessages(messages, options) {
    return messages.map((m) => ({
      content: m.content,
      name: m.name,
      role: this.mapBaseMessageToRole(m),
      functionCall: options.function_call || m.additional_kwargs.function_call
    }));
  }
  /**
   * Maps BTP LLM Proxy messages to LangChain's {@link ChatResult}
   */
  mapBTPLLMProxyMessagesToChatResult(res) {
    var _a, _b, _c;
    return {
      generations: res.choices.map((c) => ({
        text: c.message.content || "",
        message: new schema.AIMessage({
          content: c.message.content || "",
          name: c.message.role,
          additional_kwargs: {
            finish_reason: c.finishReason,
            index: c.index,
            function_call: c.message.functionCall
            // add `function_call` parameter
          }
        }),
        generationInfo: {
          finish_reason: c.finishReason,
          index: c.index,
          function_call: c.message.functionCall
          // add `function_call` parameter
        }
      })),
      llmOutput: {
        created: res.created,
        model: res.model,
        tokenUsage: {
          completionTokens: (_a = res.usage) == null ? void 0 : _a.completionTokens,
          promptTokens: (_b = res.usage) == null ? void 0 : _b.promptTokens,
          totalTokens: (_c = res.usage) == null ? void 0 : _c.totalTokens
        }
      }
    };
  }
}

exports.BTPLLMProxyChat = BTPLLMProxyChat;
