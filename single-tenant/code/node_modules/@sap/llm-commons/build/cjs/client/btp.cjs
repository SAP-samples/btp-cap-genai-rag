'use strict';

var index = require('../index-2134ea38.cjs');
var utils = require('../utils-b80ecc7f.cjs');
var client_ai21 = require('./ai21.cjs');
var client_alephalpha = require('./alephalpha.cjs');
var client_amazon = require('./amazon.cjs');
var client_anthropic = require('./anthropic.cjs');
var client_cohere = require('./cohere.cjs');
var base = require('../base-921f7eb6.cjs');
var client_google = require('./google.cjs');
var client_huggingface = require('./huggingface.cjs');
var client_openai = require('./openai.cjs');
require('@sap-cloud-sdk/util');
require('@sap/xsenv');
require('fs');
require('util');
require('axios');

const DEFAULT_MAX_TOKEN = 256;
const PROMPT_HUMAN = "\nHuman";
const PROMPT_AI = "\nAI";
const serializeMessagesToText = (messages) => {
  var _a;
  const systemText = (_a = messages.find((m) => m.role === "system")) == null ? void 0 : _a.content;
  const text = messages.filter((m) => m.role !== "system").map((message) => `${message.role === "assistant" ? PROMPT_AI : PROMPT_HUMAN}: ${message.content}`).join("");
  return `${systemText}${text}${PROMPT_AI}: `;
};
const serializeMessagesToLLaMaText = (messages) => {
  var _a;
  const systemText = `<s>[INST] <<SYS>>
${(_a = messages.find((m) => m.role === "system")) == null ? void 0 : _a.content}
<</SYS>>

`;
  const text = messages.filter((m) => m.role !== "system").map((message) => message.role === "user" ? `${message.content} [/INST]` : `${message.content} </s><s>[INST]`).join(" ");
  return `${systemText}${text}`;
};
const deserializeTextToMessage = (text) => ({
  role: "assistant",
  content: text
});
class BTPLLMProxyClient extends base.BaseLLMClient {
  /**
   * @param httpClient HTTP Client
   */
  constructor(httpClient) {
    super("btp-llm-proxy", httpClient);
  }
  /**
   * Creates a Text Completion
   *
   * **Note**: This is just a wrapper around text/chat completion with underlying LLM client.
   *
   * @param params the payload to send to the API
   * @param requestConfig Axios Request Configuration
   *
   * @see https://axios-http.com/docs/req_config
   * @see https://axios-http.com/docs/cancellation
   *
   * @returns HTTP Response data as JSON
   * @throws {@link BTPLLMError} in case of HTTP errors or unknown `deployment_id`
   */
  async createTextCompletion(params, requestConfig) {
    const deploymentId = params.deployment_id;
    if (deploymentId === "text-davinci-003" || deploymentId === "code-davinci-002" || deploymentId === "gpt-35-turbo" || deploymentId === "gpt-35-turbo-16k" || deploymentId === "gpt-4" || deploymentId === "gpt-4-32k") {
      return this.createTextCompletionViaOpenAIGPTClient(params, requestConfig);
    } else if (deploymentId === "gcp-text-bison-001" || deploymentId === "gcp-chat-bison-001") {
      return this.createTextCompletionViaGooglePaLMClient(params, requestConfig);
    } else if (deploymentId === "anthropic-claude-v1" || deploymentId === "anthropic-claude-instant-v1" || deploymentId === "anthropic-claude-v2") {
      return this.createTextCompletionViaAnthropicClaudeClient(params, requestConfig);
    } else if (deploymentId === "cohere-command") {
      return this.createTextCompletionViaCohereCommandClient(params, requestConfig);
    } else if (deploymentId === "amazon-titan-tg1-large") {
      return this.createTextCompletionViaAmazonTitanClient(params, requestConfig);
    } else if (deploymentId === "alephalpha") {
      return this.createTextCompletionViaAlephAlphaLuminousClient(params, requestConfig);
    } else if (deploymentId === "ai21-j2-grande-instruct" || deploymentId === "ai21-j2-jumbo-instruct") {
      return this.createTextCompletionViaAI21JurassicClient(params, requestConfig);
    } else if (deploymentId === "falcon-7b" || deploymentId === "falcon-40b-instruct" || deploymentId === "llama2-13b-chat-hf" || deploymentId === "llama2-70b-chat-hf") {
      return this.createTextCompletionViaHuggingFaceClient(params, requestConfig);
    } else {
      throw new index.BTPLLMError("Cannot infer LLM model from `deployment_id`");
    }
  }
  /**
   * Creates a Chat Completion
   *
   * **Note**: This is just a wrapper around text/chat completion with underlying LLM client.
   *
   * @param params the payload to send to the API
   * @param requestConfig Axios Request Configuration
   *
   * @see https://axios-http.com/docs/req_config
   * @see https://axios-http.com/docs/cancellation
   *
   * @returns HTTP Response data as JSON
   * @throws {@link BTPLLMError} in case of HTTP errors or unknown `deployment_id`
   */
  async createChatCompletion(params, requestConfig) {
    const deploymentId = params.deployment_id;
    if (deploymentId === "text-davinci-003" || deploymentId === "code-davinci-002" || deploymentId === "gpt-35-turbo" || deploymentId === "gpt-35-turbo-16k" || deploymentId === "gpt-4" || deploymentId === "gpt-4-32k") {
      return this.createChatCompletionViaOpenAIGPTClient(params, requestConfig);
    } else if (deploymentId === "gcp-text-bison-001" || deploymentId === "gcp-chat-bison-001") {
      return this.createChatCompletionViaGooglePaLMClient(params, requestConfig);
    } else if (deploymentId === "anthropic-claude-v1" || deploymentId === "anthropic-claude-instant-v1" || deploymentId === "anthropic-claude-v2") {
      return this.createChatCompletionViaAnthropicClaudeClient(params, requestConfig);
    } else if (deploymentId === "cohere-command") {
      return this.createChatCompletionViaCohereCommandClient(params, requestConfig);
    } else if (deploymentId === "amazon-titan-tg1-large") {
      return this.createChatCompletionViaAmazonTitanClient(params, requestConfig);
    } else if (deploymentId === "alephalpha") {
      return this.createChatCompletionViaAlephAlphaLuminousClient(params, requestConfig);
    } else if (deploymentId === "ai21-j2-grande-instruct" || deploymentId === "ai21-j2-jumbo-instruct") {
      return this.createChatCompletionViaAI21JurassicClient(params, requestConfig);
    } else if (deploymentId === "falcon-7b" || deploymentId === "falcon-40b-instruct" || deploymentId === "llama2-13b-chat-hf" || deploymentId === "llama2-70b-chat-hf") {
      return this.createChatCompletionViaHuggingFaceClient(params, requestConfig);
    } else {
      throw new index.BTPLLMError("Cannot infer LLM model from `deployment_id`");
    }
  }
  /**
   * Creates an Embedding
   *
   * **Note**: This is just a wrapper around embedding with underlying LLM client.
   *
   * @param params the payload to send to the API
   * @param requestConfig Axios Request Configuration
   *
   * @see https://axios-http.com/docs/req_config
   * @see https://axios-http.com/docs/cancellation
   *
   * @returns HTTP Response data as JSON
   * @throws {@link BTPLLMError} in case of HTTP errors
   */
  async createEmbedding(params, requestConfig) {
    const deploymentId = params.deployment_id;
    if (deploymentId === "text-embedding-ada-002-v2") {
      return this.createEmbeddingViaOpenAIGPTClient(params, requestConfig);
    } else if (deploymentId === "gcp-textembedding-gecko-001") {
      return this.createEmbeddingViaGooglePaLMClient(params, requestConfig);
    } else if (deploymentId === "amazon-titan-e1t-medium") {
      return this.createEmbeddingViaAmazonTitanClient(params, requestConfig);
    } else {
      throw new index.BTPLLMError("Cannot infer LLM model from `deployment_id`");
    }
  }
  /// / / / / / / / / ///
  /// TEXT COMPLETION ///
  /// / / / / / / / / ///
  /**
   * Creates a Text completion via OpenAI GPT client
   */
  async createTextCompletionViaOpenAIGPTClient(params, requestConfig) {
    const deploymentId = params.deployment_id;
    const openaiClient = new client_openai.BTPOpenAIGPTClient(this.httpClient);
    if (deploymentId === "text-davinci-003" || deploymentId === "code-davinci-002") {
      const res = await openaiClient.createCompletion(
        {
          deployment_id: deploymentId,
          prompt: params.prompt,
          max_tokens: params.maxOutputTokens,
          temperature: utils.normalizeRange(params.temperature, 0, 2),
          top_p: utils.normalizeRange(params.topP, 0, 1),
          n: params.n,
          stop: params.stop
        },
        requestConfig
      );
      return {
        model: deploymentId,
        created: Date.now(),
        choices: res.choices.map((c) => {
          var _a;
          return {
            finishReason: (_a = c.finish_reason) != null ? _a : void 0,
            index: c.index,
            text: c.text
          };
        }),
        usage: {
          completionTokens: res.usage.completion_tokens,
          promptTokens: res.usage.prompt_tokens,
          totalTokens: res.usage.total_tokens
        }
      };
    } else if (deploymentId === "gpt-35-turbo" || deploymentId === "gpt-35-turbo-16k" || deploymentId === "gpt-4" || deploymentId === "gpt-4-32k") {
      const res = await openaiClient.createChatCompletion(
        {
          deployment_id: deploymentId,
          messages: [
            {
              role: "user",
              content: params.prompt
            }
          ],
          max_tokens: params.maxOutputTokens,
          temperature: utils.normalizeRange(params.temperature, 0, 2),
          top_p: utils.normalizeRange(params.topP, 0, 1),
          n: params.n,
          stop: params.stop
        },
        requestConfig
      );
      return {
        model: deploymentId,
        created: Date.now(),
        choices: res.choices.map((c) => {
          var _a, _b;
          return {
            finishReason: (_a = c.finish_reason) != null ? _a : void 0,
            index: c.index,
            text: (_b = c.message.content) != null ? _b : ""
          };
        }),
        usage: {
          completionTokens: res.usage.completion_tokens,
          promptTokens: res.usage.prompt_tokens,
          totalTokens: res.usage.total_tokens
        }
      };
    } else {
      throw new index.BTPLLMError("Cannot infer OpenAI GPT model from `deployment_id`");
    }
  }
  /**
   * Creates a Text completion via Google PaLM client
   */
  async createTextCompletionViaGooglePaLMClient(params, requestConfig) {
    var _a, _b;
    const deploymentId = params.deployment_id;
    const googlePalmClient = new client_google.BTPGooglePaLMClient(this.httpClient);
    if (deploymentId === "gcp-text-bison-001") {
      const res = await googlePalmClient.predictText(
        {
          deployment_id: deploymentId,
          instances: [
            {
              prompt: params.prompt
            }
          ],
          parameters: {
            maxOutputTokens: (_a = params.maxOutputTokens) != null ? _a : DEFAULT_MAX_TOKEN,
            temperature: utils.normalizeRange(params.temperature, 0, 1),
            topP: utils.normalizeRange(params.topP, 0, 1),
            topK: params.topK
          }
        },
        requestConfig
      );
      return {
        model: deploymentId,
        created: Date.now(),
        choices: res.predictions.map((p, idx) => ({
          index: idx,
          text: p.content
        })),
        usage: {
          completionTokens: res.metadata.tokenMetadata.inputTokenCount.totalTokens,
          promptTokens: res.metadata.tokenMetadata.outputTokenCount.totalTokens,
          totalTokens: res.metadata.tokenMetadata.inputTokenCount.totalTokens + res.metadata.tokenMetadata.outputTokenCount.totalTokens
        }
      };
    } else if (deploymentId === "gcp-chat-bison-001") {
      const res = await googlePalmClient.predictChat(
        {
          deployment_id: deploymentId,
          instances: [
            {
              messages: [
                {
                  author: "human",
                  content: params.prompt
                }
              ]
            }
          ],
          parameters: {
            maxOutputTokens: (_b = params.maxOutputTokens) != null ? _b : DEFAULT_MAX_TOKEN,
            temperature: utils.normalizeRange(params.temperature, 0, 1),
            topP: utils.normalizeRange(params.topP, 0, 1),
            topK: params.topK
          }
        },
        requestConfig
      );
      return {
        model: deploymentId,
        created: Date.now(),
        choices: res.predictions.map((p, idx) => ({
          index: idx,
          text: p.candidates[0].content
        })),
        usage: {
          completionTokens: res.metadata.tokenMetadata.inputTokenCount.totalTokens,
          promptTokens: res.metadata.tokenMetadata.outputTokenCount.totalTokens,
          totalTokens: res.metadata.tokenMetadata.inputTokenCount.totalTokens + res.metadata.tokenMetadata.outputTokenCount.totalTokens
        }
      };
    } else {
      throw new index.BTPLLMError("Cannot infer Google PaLM model from `deployment_id`");
    }
  }
  /**
   * Creates a Text completion via Anthropic Claude client
   */
  async createTextCompletionViaAnthropicClaudeClient(params, requestConfig) {
    var _a, _b;
    const deploymentId = params.deployment_id;
    const anthropicClaudeClient = new client_anthropic.BTPAnthropicClaudeClient(this.httpClient);
    if (deploymentId === "anthropic-claude-v1" || deploymentId === "anthropic-claude-instant-v1" || deploymentId === "anthropic-claude-v2") {
      const res = await anthropicClaudeClient.createChatCompletion(
        {
          deployment_id: deploymentId,
          messages: [
            {
              role: "Human",
              content: params.prompt
            }
          ],
          max_tokens_to_sample: (_a = params.maxOutputTokens) != null ? _a : DEFAULT_MAX_TOKEN,
          temperature: utils.normalizeRange(params.temperature, 0, 1),
          top_p: utils.normalizeRange(params.topP, 0, 1),
          top_k: params.topK,
          stop_sequences: (_b = params.stop) != null ? _b : ["\n\n"]
        },
        requestConfig
      );
      return {
        model: deploymentId,
        created: Date.now(),
        choices: [
          {
            index: 0,
            text: res.completion,
            finishReason: res.stop_reason
          }
        ]
      };
    } else {
      throw new index.BTPLLMError("Cannot infer Anthropic Claude model from `deployment_id`");
    }
  }
  /**
   * Creates a Text completion via Cohere Command client
   */
  async createTextCompletionViaCohereCommandClient(params, requestConfig) {
    var _a, _b, _c;
    const deploymentId = params.deployment_id;
    const cohereCommandClient = new client_cohere.BTPCohereCommandClient(this.httpClient);
    if (deploymentId === "cohere-command") {
      const res = await cohereCommandClient.createTextCompletion(
        {
          deployment_id: deploymentId,
          prompt: params.prompt,
          max_tokens: (_a = params.maxOutputTokens) != null ? _a : DEFAULT_MAX_TOKEN,
          temperature: utils.normalizeRange(params.temperature, 0, 5),
          p: utils.normalizeRange(params.topP, 0.01, 0.99),
          k: params.topK,
          stop_sequences: (_b = params.stop) != null ? _b : ["\n\n"],
          num_generations: (_c = params.n) != null ? _c : 1
        },
        requestConfig
      );
      return {
        model: deploymentId,
        created: Date.now(),
        choices: res.generations.map((r, idx) => {
          var _a2;
          return {
            index: (_a2 = r.index) != null ? _a2 : idx,
            text: r.text
          };
        })
      };
    } else {
      throw new index.BTPLLMError("Cannot infer Cohere Command model from `deployment_id`");
    }
  }
  /**
   * Creates a Text completion via Amazon Titan client
   */
  async createTextCompletionViaAmazonTitanClient(params, requestConfig) {
    const deploymentId = params.deployment_id;
    const amazonTitanClient = new client_amazon.BTPAmazonTitanClient(this.httpClient);
    if (deploymentId === "amazon-titan-tg1-large") {
      const res = await amazonTitanClient.createTextCompletion(
        {
          deployment_id: deploymentId,
          inputText: params.prompt,
          textGenerationConfig: {
            maxTokenCount: params.maxOutputTokens,
            temperature: utils.normalizeRange(params.temperature, 0, 1),
            topP: utils.normalizeRange(params.topP, 0, 1),
            stopSequences: params.stop
          }
        },
        requestConfig
      );
      return {
        model: deploymentId,
        created: Date.now(),
        choices: res.results.map((r, idx) => ({
          index: idx,
          text: r.outputText,
          finishReason: r.completionReason
        })),
        usage: {
          completionTokens: res.results.reduce((acc, val) => acc + val.tokenCount, 0),
          promptTokens: res.inputTextTokenCount,
          totalTokens: res.inputTextTokenCount + res.results.reduce((acc, val) => acc + val.tokenCount, 0)
        }
      };
    } else {
      throw new index.BTPLLMError("Cannot infer Amazon Titan model from `deployment_id`");
    }
  }
  /**
   * Creates a Text completion via AlephAlpha Luminous client
   */
  async createTextCompletionViaAlephAlphaLuminousClient(params, requestConfig) {
    var _a, _b;
    const deploymentId = params.deployment_id;
    const alephAlphaLuminousClient = new client_alephalpha.BTPAlephAlphaLuminousClient(this.httpClient);
    if (deploymentId === "alephalpha") {
      const res = await alephAlphaLuminousClient.createTextCompletion(
        {
          deployment_id: deploymentId,
          prompt: params.prompt,
          maximum_tokens: (_a = params.maxOutputTokens) != null ? _a : DEFAULT_MAX_TOKEN,
          temperature: utils.normalizeRange(params.temperature, 0, 1),
          top_p: utils.normalizeRange(params.topP, 0, 1),
          top_k: params.topK,
          n: params.n,
          stop_sequences: (_b = params.stop) != null ? _b : ["\n\n"],
          echo: false
        },
        requestConfig
      );
      return {
        model: deploymentId,
        created: Date.now(),
        choices: res.completions.map((c, idx) => ({
          index: idx,
          text: c.completion,
          finishReason: c.finish_reason
        })),
        usage: {
          completionTokens: res.completions.reduce((acc, val) => {
            var _a2, _b2;
            return acc + ((_b2 = (_a2 = val.completion_tokens) == null ? void 0 : _a2.length) != null ? _b2 : 0);
          }, 0)
        }
      };
    } else {
      throw new index.BTPLLMError("Cannot infer AlephAlpha Luminous model from `deployment_id`");
    }
  }
  /**
   * Creates a Text completion via AI21 Jurassic client
   */
  async createTextCompletionViaAI21JurassicClient(params, requestConfig) {
    var _a, _b;
    const deploymentId = params.deployment_id;
    const ai21JurassicClient = new client_ai21.BTPAI21JurassicClient(this.httpClient);
    if (deploymentId === "ai21-j2-grande-instruct" || deploymentId === "ai21-j2-jumbo-instruct") {
      const res = await ai21JurassicClient.createTextCompletion(
        {
          deployment_id: deploymentId,
          prompt: params.prompt,
          maxTokens: params.maxOutputTokens,
          temperature: utils.normalizeRange(params.temperature, 0, 1),
          topP: utils.normalizeRange(params.topP, 0, 1),
          topKReturn: params.topK,
          numResults: params.n,
          stopSequences: params.stop
        },
        requestConfig
      );
      return {
        model: deploymentId,
        created: Date.now(),
        choices: res.completions.map((c, idx) => ({
          index: idx,
          text: c.data.text,
          finishReason: c.finishReason.reason
        })),
        usage: {
          completionTokens: res.completions.reduce((acc, val) => {
            var _a2, _b2;
            return acc + ((_b2 = (_a2 = val.data.tokens) == null ? void 0 : _a2.length) != null ? _b2 : 0);
          }, 0),
          promptTokens: (_a = res.prompt.tokens) == null ? void 0 : _a.length,
          totalTokens: ((_b = res.prompt.tokens) == null ? void 0 : _b.length) + res.completions.reduce((acc, val) => {
            var _a2, _b2;
            return acc + ((_b2 = (_a2 = val.data.tokens) == null ? void 0 : _a2.length) != null ? _b2 : 0);
          }, 0)
        }
      };
    } else {
      throw new index.BTPLLMError("Cannot infer AI21 Jurassic model from `deployment_id`");
    }
  }
  /**
   * Creates a Text completion via HuggingFace client
   */
  async createTextCompletionViaHuggingFaceClient(params, requestConfig) {
    const deploymentId = params.deployment_id;
    const huggingFaceClient = new client_huggingface.BTPHuggingFaceClient(this.httpClient);
    if (deploymentId === "falcon-7b" || deploymentId === "falcon-40b-instruct" || deploymentId === "llama2-13b-chat-hf" || deploymentId === "llama2-70b-chat-hf") {
      const res = await huggingFaceClient.textGeneration(
        {
          deployment_id: deploymentId,
          inputs: params.prompt,
          parameters: {
            return_full_text: false,
            max_new_tokens: params.maxOutputTokens,
            temperature: utils.normalizeRange(params.temperature, 0, 100),
            top_p: utils.normalizeRange(params.topP, 0, 1.00000010000001),
            top_k: params.topK
          }
        },
        requestConfig
      );
      return {
        model: deploymentId,
        created: Date.now(),
        choices: [
          {
            index: 0,
            text: res.generated_text
          }
        ]
      };
    } else {
      throw new index.BTPLLMError("Cannot infer HuggingFace model from `deployment_id`");
    }
  }
  /// / / / / / / / / ///
  /// CHAT COMPLETION ///
  /// / / / / / / / / ///
  /**
   * Creates a Chat completion via OpenAI GPT client
   */
  async createChatCompletionViaOpenAIGPTClient(params, requestConfig) {
    var _a;
    const deploymentId = params.deployment_id;
    const openaiClient = new client_openai.BTPOpenAIGPTClient(this.httpClient);
    if (deploymentId === "text-davinci-003" || deploymentId === "code-davinci-002") {
      const res = await openaiClient.createCompletion(
        {
          deployment_id: deploymentId,
          prompt: serializeMessagesToText(params.messages),
          max_tokens: params.maxOutputTokens,
          temperature: utils.normalizeRange(params.temperature, 0, 2),
          top_p: utils.normalizeRange(params.topP, 0, 1),
          n: params.n,
          stop: (_a = params.stop) != null ? _a : [PROMPT_HUMAN]
        },
        requestConfig
      );
      return {
        model: deploymentId,
        created: Date.now(),
        choices: res.choices.map((c) => {
          var _a2;
          return {
            finishReason: (_a2 = c.finish_reason) != null ? _a2 : void 0,
            index: c.index,
            message: deserializeTextToMessage(c.text)
          };
        }),
        usage: {
          completionTokens: res.usage.completion_tokens,
          promptTokens: res.usage.prompt_tokens,
          totalTokens: res.usage.total_tokens
        }
      };
    } else if (deploymentId === "gpt-35-turbo" || deploymentId === "gpt-35-turbo-16k" || deploymentId === "gpt-4" || deploymentId === "gpt-4-32k") {
      const res = await openaiClient.createChatCompletion(
        {
          deployment_id: deploymentId,
          messages: params.messages.map((m) => ({
            role: m.role,
            content: m.content,
            name: m.name,
            function_call: m.functionCall
          })),
          max_tokens: params.maxOutputTokens,
          temperature: utils.normalizeRange(params.temperature, 0, 2),
          top_p: utils.normalizeRange(params.topP, 0, 1),
          n: params.n,
          stop: params.stop,
          functions: params.functions
        },
        requestConfig
      );
      return {
        model: deploymentId,
        created: Date.now(),
        choices: res.choices.map((c) => {
          var _a2;
          return {
            finishReason: (_a2 = c.finish_reason) != null ? _a2 : void 0,
            index: c.index,
            message: {
              role: c.message.role,
              content: c.message.content,
              functionCall: c.message.function_call,
              name: c.message.name
            }
          };
        }),
        usage: {
          completionTokens: res.usage.completion_tokens,
          promptTokens: res.usage.prompt_tokens,
          totalTokens: res.usage.total_tokens
        }
      };
    } else {
      throw new index.BTPLLMError("Cannot infer OpenAI GPT model from `deployment_id`");
    }
  }
  /**
   * Creates a Chat completion via Google PaLM client
   */
  async createChatCompletionViaGooglePaLMClient(params, requestConfig) {
    var _a, _b, _c, _d;
    const deploymentId = params.deployment_id;
    const googlePalmClient = new client_google.BTPGooglePaLMClient(this.httpClient);
    if (deploymentId === "gcp-text-bison-001") {
      const res = await googlePalmClient.predictText(
        {
          deployment_id: deploymentId,
          instances: [
            {
              prompt: serializeMessagesToText(params.messages)
            }
          ],
          parameters: {
            maxOutputTokens: (_a = params.maxOutputTokens) != null ? _a : DEFAULT_MAX_TOKEN,
            temperature: utils.normalizeRange(params.temperature, 0, 1),
            topP: utils.normalizeRange(params.topP, 0, 1),
            topK: params.topK
          }
        },
        requestConfig
      );
      return {
        model: deploymentId,
        created: Date.now(),
        choices: res.predictions.map((p, idx) => ({
          index: idx,
          message: deserializeTextToMessage(p.content)
        })),
        usage: {
          completionTokens: res.metadata.tokenMetadata.inputTokenCount.totalTokens,
          promptTokens: res.metadata.tokenMetadata.outputTokenCount.totalTokens,
          totalTokens: res.metadata.tokenMetadata.inputTokenCount.totalTokens + res.metadata.tokenMetadata.outputTokenCount.totalTokens
        }
      };
    } else if (deploymentId === "gcp-chat-bison-001") {
      const res = await googlePalmClient.predictChat(
        {
          deployment_id: deploymentId,
          instances: [
            {
              context: (_c = (_b = params.messages.find((m) => m.role === "system")) == null ? void 0 : _b.content) != null ? _c : "",
              messages: params.messages.filter((m) => m.role !== "system").map((m) => {
                var _a2;
                return {
                  author: m.role,
                  content: (_a2 = m.content) != null ? _a2 : ""
                };
              })
            }
          ],
          parameters: {
            maxOutputTokens: (_d = params.maxOutputTokens) != null ? _d : DEFAULT_MAX_TOKEN,
            temperature: utils.normalizeRange(params.temperature, 0, 1),
            topP: utils.normalizeRange(params.topP, 0, 1),
            topK: params.topK
          }
        },
        requestConfig
      );
      return {
        model: deploymentId,
        created: Date.now(),
        choices: res.predictions.map((p, idx) => ({
          index: idx,
          message: {
            role: "assistant",
            content: p.candidates[0].content,
            name: p.candidates[0].author
          }
        })),
        usage: {
          completionTokens: res.metadata.tokenMetadata.inputTokenCount.totalTokens,
          promptTokens: res.metadata.tokenMetadata.outputTokenCount.totalTokens,
          totalTokens: res.metadata.tokenMetadata.inputTokenCount.totalTokens + res.metadata.tokenMetadata.outputTokenCount.totalTokens
        }
      };
    } else {
      throw new index.BTPLLMError("Cannot infer Google PaLM model from `deployment_id`");
    }
  }
  /**
   * Creates a Chat completion via Anthropic Claude client
   */
  async createChatCompletionViaAnthropicClaudeClient(params, requestConfig) {
    var _a;
    const deploymentId = params.deployment_id;
    const anthropicClaudeClient = new client_anthropic.BTPAnthropicClaudeClient(this.httpClient);
    if (deploymentId === "anthropic-claude-v1" || deploymentId === "anthropic-claude-instant-v1" || deploymentId === "anthropic-claude-v2") {
      const res = await anthropicClaudeClient.createChatCompletion(
        {
          deployment_id: deploymentId,
          messages: params.messages.map((m) => {
            var _a2;
            return {
              content: (_a2 = m.content) != null ? _a2 : "",
              role: m.role === "user" ? "Human" : "Assistant"
            };
          }),
          max_tokens_to_sample: (_a = params.maxOutputTokens) != null ? _a : DEFAULT_MAX_TOKEN,
          temperature: utils.normalizeRange(params.temperature, 0, 1),
          top_p: utils.normalizeRange(params.topP, 0, 1),
          top_k: params.topK,
          stop_sequences: params.stop
        },
        requestConfig
      );
      return {
        model: deploymentId,
        created: Date.now(),
        choices: [
          {
            index: 0,
            message: deserializeTextToMessage(res.completion),
            finishReason: res.stop_reason
          }
        ]
      };
    } else {
      throw new index.BTPLLMError("Cannot infer Anthropic Claude model from `deployment_id`");
    }
  }
  /**
   * Creates a Chat completion via Cohere Command client
   */
  async createChatCompletionViaCohereCommandClient(params, requestConfig) {
    var _a, _b, _c;
    const deploymentId = params.deployment_id;
    const cohereCommandClient = new client_cohere.BTPCohereCommandClient(this.httpClient);
    if (deploymentId === "cohere-command") {
      const res = await cohereCommandClient.createTextCompletion(
        {
          deployment_id: deploymentId,
          prompt: serializeMessagesToText(params.messages),
          max_tokens: (_a = params.maxOutputTokens) != null ? _a : DEFAULT_MAX_TOKEN,
          temperature: utils.normalizeRange(params.temperature, 0, 5),
          p: utils.normalizeRange(params.topP, 0.01, 0.99),
          k: params.topK,
          stop_sequences: (_b = params.stop) != null ? _b : ["\n\n"],
          num_generations: (_c = params.n) != null ? _c : 1
        },
        requestConfig
      );
      return {
        model: deploymentId,
        created: Date.now(),
        choices: res.generations.map((r, idx) => {
          var _a2;
          return {
            index: (_a2 = r.index) != null ? _a2 : idx,
            message: deserializeTextToMessage(r.text)
          };
        })
      };
    } else {
      throw new index.BTPLLMError("Cannot infer Cohere Command model from `deployment_id`");
    }
  }
  /**
   * Creates a Chat completion via Amazon Titan client
   */
  async createChatCompletionViaAmazonTitanClient(params, requestConfig) {
    const deploymentId = params.deployment_id;
    const amazonTitanClient = new client_amazon.BTPAmazonTitanClient(this.httpClient);
    if (deploymentId === "amazon-titan-tg1-large") {
      const res = await amazonTitanClient.createTextCompletion(
        {
          deployment_id: deploymentId,
          inputText: serializeMessagesToText(params.messages),
          textGenerationConfig: {
            maxTokenCount: params.maxOutputTokens,
            temperature: utils.normalizeRange(params.temperature, 0, 1),
            topP: utils.normalizeRange(params.topP, 0, 1),
            stopSequences: params.stop
          }
        },
        requestConfig
      );
      return {
        model: deploymentId,
        created: Date.now(),
        choices: res.results.map((r, idx) => ({
          index: idx,
          message: deserializeTextToMessage(r.outputText),
          finishReason: r.completionReason
        })),
        usage: {
          completionTokens: res.results.reduce((acc, val) => acc + val.tokenCount, 0),
          promptTokens: res.inputTextTokenCount,
          totalTokens: res.inputTextTokenCount + res.results.reduce((acc, val) => acc + val.tokenCount, 0)
        }
      };
    } else {
      throw new index.BTPLLMError("Cannot infer Amazon Titan model from `deployment_id`");
    }
  }
  /**
   * Creates a Chat completion via AlephAlpha Luminous client
   */
  async createChatCompletionViaAlephAlphaLuminousClient(params, requestConfig) {
    var _a, _b;
    const deploymentId = params.deployment_id;
    const alephAlphaLuminousClient = new client_alephalpha.BTPAlephAlphaLuminousClient(this.httpClient);
    if (deploymentId === "alephalpha") {
      const res = await alephAlphaLuminousClient.createTextCompletion(
        {
          deployment_id: deploymentId,
          prompt: serializeMessagesToText(params.messages),
          maximum_tokens: (_a = params.maxOutputTokens) != null ? _a : DEFAULT_MAX_TOKEN,
          temperature: utils.normalizeRange(params.temperature, 0, 1),
          top_p: utils.normalizeRange(params.topP, 0, 1),
          top_k: params.topK,
          n: params.n,
          stop_sequences: (_b = params.stop) != null ? _b : ["\n"],
          // stop tokens from AlephAlpha docs
          echo: false
        },
        requestConfig
      );
      return {
        model: deploymentId,
        created: Date.now(),
        choices: res.completions.map((c, idx) => ({
          index: idx,
          message: deserializeTextToMessage(c.completion),
          finishReason: c.finish_reason
        })),
        usage: {
          completionTokens: res.completions.reduce((acc, val) => {
            var _a2, _b2;
            return acc + ((_b2 = (_a2 = val.completion_tokens) == null ? void 0 : _a2.length) != null ? _b2 : 0);
          }, 0)
        }
      };
    } else {
      throw new index.BTPLLMError("Cannot infer AlephAlpha Luminous model from `deployment_id`");
    }
  }
  /**
   * Creates a Chat completion via AI21 Jurassic client
   */
  async createChatCompletionViaAI21JurassicClient(params, requestConfig) {
    var _a, _b, _c;
    const deploymentId = params.deployment_id;
    const ai21JurassicClient = new client_ai21.BTPAI21JurassicClient(this.httpClient);
    if (deploymentId === "ai21-j2-grande-instruct" || deploymentId === "ai21-j2-jumbo-instruct") {
      const res = await ai21JurassicClient.createTextCompletion(
        {
          deployment_id: deploymentId,
          prompt: serializeMessagesToText(params.messages),
          maxTokens: params.maxOutputTokens,
          temperature: utils.normalizeRange(params.temperature, 0, 1),
          topP: utils.normalizeRange(params.topP, 0, 1),
          topKReturn: params.topK,
          numResults: params.n,
          stopSequences: (_a = params.stop) != null ? _a : [PROMPT_HUMAN]
        },
        requestConfig
      );
      return {
        model: deploymentId,
        created: Date.now(),
        choices: res.completions.map((c, idx) => ({
          index: idx,
          message: deserializeTextToMessage(c.data.text),
          finishReason: c.finishReason.reason
        })),
        usage: {
          completionTokens: res.completions.reduce((acc, val) => {
            var _a2, _b2;
            return acc + ((_b2 = (_a2 = val.data.tokens) == null ? void 0 : _a2.length) != null ? _b2 : 0);
          }, 0),
          promptTokens: (_b = res.prompt.tokens) == null ? void 0 : _b.length,
          totalTokens: ((_c = res.prompt.tokens) == null ? void 0 : _c.length) + res.completions.reduce((acc, val) => {
            var _a2, _b2;
            return acc + ((_b2 = (_a2 = val.data.tokens) == null ? void 0 : _a2.length) != null ? _b2 : 0);
          }, 0)
        }
      };
    } else {
      throw new index.BTPLLMError("Cannot infer AI21 Jurassic model from `deployment_id`");
    }
  }
  /**
   * Creates a Chat completion via HuggingFace client
   */
  async createChatCompletionViaHuggingFaceClient(params, requestConfig) {
    var _a;
    const deploymentId = params.deployment_id;
    const huggingFaceClient = new client_huggingface.BTPHuggingFaceClient(this.httpClient);
    if (deploymentId === "falcon-7b" || deploymentId === "falcon-40b-instruct" || deploymentId === "llama2-13b-chat-hf" || deploymentId === "llama2-70b-chat-hf") {
      const res = await huggingFaceClient.textGeneration(
        {
          deployment_id: deploymentId,
          inputs: deploymentId.startsWith("llama2") ? serializeMessagesToLLaMaText(params.messages) : serializeMessagesToText(params.messages),
          parameters: {
            return_full_text: false,
            max_new_tokens: params.maxOutputTokens,
            temperature: utils.normalizeRange(params.temperature, 0, 100),
            top_p: utils.normalizeRange(params.topP, 0, 1.00000010000001),
            top_k: params.topK,
            stop: (_a = params.stop) != null ? _a : [PROMPT_HUMAN, "<|endoftext|>"]
          }
        },
        requestConfig
      );
      return {
        model: deploymentId,
        created: Date.now(),
        choices: [
          {
            index: 0,
            message: deserializeTextToMessage(res.generated_text)
          }
        ]
      };
    } else {
      throw new index.BTPLLMError("Cannot infer HuggingFace model from `deployment_id`");
    }
  }
  /// / / / / / ///
  /// EMBEDDING ///
  /// / / / / / ///
  /**
   * Creates embedding via OpenAI GPT client
   */
  async createEmbeddingViaOpenAIGPTClient(params, requestConfig) {
    const deploymentId = params.deployment_id;
    const openaiGPTClient = new client_openai.BTPOpenAIGPTClient(this.httpClient);
    if (deploymentId === "text-embedding-ada-002-v2") {
      const res = await openaiGPTClient.createEmbedding(
        {
          deployment_id: deploymentId,
          input: params.input
        },
        requestConfig
      );
      return {
        model: deploymentId,
        data: res.data.map((d) => ({
          index: d.index,
          embedding: d.embedding
        })),
        usage: {
          promptTokens: res.usage.prompt_tokens
        }
      };
    } else {
      throw new index.BTPLLMError("Cannot infer HuggingFace model from `deployment_id`");
    }
  }
  /**
   * Creates embedding via Google PaLM client
   */
  async createEmbeddingViaGooglePaLMClient(params, requestConfig) {
    const deploymentId = params.deployment_id;
    const googlePalmClient = new client_google.BTPGooglePaLMClient(this.httpClient);
    if (deploymentId === "gcp-textembedding-gecko-001") {
      const res = await googlePalmClient.embedText(
        {
          deployment_id: deploymentId,
          instances: [
            {
              content: params.input
            }
          ]
        },
        requestConfig
      );
      return {
        model: deploymentId,
        data: res.predictions.map((p, idx) => ({
          index: idx,
          embedding: p.embeddings.values
        })),
        usage: {
          promptTokens: res.predictions[0].embeddings.statistics.token_count
        }
      };
    } else {
      throw new index.BTPLLMError("Cannot infer HuggingFace model from `deployment_id`");
    }
  }
  /**
   * Creates embedding via Amazon Titan client
   */
  async createEmbeddingViaAmazonTitanClient(params, requestConfig) {
    const deploymentId = params.deployment_id;
    const amazonTitanClient = new client_amazon.BTPAmazonTitanClient(this.httpClient);
    if (deploymentId === "amazon-titan-e1t-medium") {
      const res = await amazonTitanClient.createEmbedding(
        {
          deployment_id: deploymentId,
          inputText: params.input
        },
        requestConfig
      );
      return {
        model: deploymentId,
        data: [
          {
            index: 0,
            embedding: res.embedding
          }
        ],
        usage: {
          promptTokens: res.inputTextTokenCount
        }
      };
    } else {
      throw new index.BTPLLMError("Cannot infer HuggingFace model from `deployment_id`");
    }
  }
}

exports.BTPLLMProxyClient = BTPLLMProxyClient;
